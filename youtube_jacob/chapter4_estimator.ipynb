{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    # repeat(step_repeat_number)\n",
    "    dataset = tf.data.TextLineDataset('./up_down_dataset.csv')\\\n",
    "        .batch(20)\\\n",
    "        .repeat(9999)\\\n",
    "        .make_one_shot_iterator()\\\n",
    "        .get_next()\n",
    "    lines = tf.decode_csv(dataset, record_defaults=[[0]]*11)\n",
    "    feature = tf.stack(lines[:-1], axis=1)\n",
    "    label = tf.expand_dims(lines[-1], axis=-1)\n",
    "    \n",
    "    feature = tf.cast(feature, tf.float32)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):  \n",
    "    # estimator에서 원하는 모양이 세가지 파라미터를 똑같은 이름으로 받아야한다.\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    \n",
    "    layer1 = tf.layers.dense(features, units=10, activation=tf.nn.relu) # units = the number of node\n",
    "    layer2 = tf.layers.dense(layer1, units=10, activation=tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, units=10, activation=tf.nn.relu)\n",
    "    layer4 = tf.layers.dense(layer3, units=10, activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(layer4, units=1)  # label = 0 or 1\n",
    "    \n",
    "    if TRAIN:\n",
    "        gs = tf.train.get_global_step()\n",
    "        # loss function\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, out)\n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step=gs)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,loss=loss,train_op=train_op)\n",
    "        \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, out)\n",
    "        # prediction\n",
    "        pred = tf.nn.sigmoid(out)  # number of 0~1 \n",
    "        # accuracy\n",
    "        acc = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,loss=loss,eval_metric_ops={'acc':acc})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmk380tye\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpmk380tye', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4a3050ee80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpmk380tye/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.507619, step = 1\n",
      "INFO:tensorflow:global_step/sec: 436.285\n",
      "INFO:tensorflow:loss = 0.0241754, step = 101 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.968\n",
      "INFO:tensorflow:loss = 0.00691826, step = 201 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 535.175\n",
      "INFO:tensorflow:loss = 0.00365084, step = 301 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 657.613\n",
      "INFO:tensorflow:loss = 0.00236455, step = 401 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.866\n",
      "INFO:tensorflow:loss = 0.00170434, step = 501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 663.026\n",
      "INFO:tensorflow:loss = 0.00131173, step = 601 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.895\n",
      "INFO:tensorflow:loss = 0.00105623, step = 701 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.797\n",
      "INFO:tensorflow:loss = 0.000879026, step = 801 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.598\n",
      "INFO:tensorflow:loss = 0.000748704, step = 901 (0.158 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpmk380tye/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00065017.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-17-11:19:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpmk380tye/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-17-11:19:02\n",
      "INFO:tensorflow:Saving dict for global step 1000: acc = 1.0, global_step = 1000, loss = 0.000649282\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpmk380tye/model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':  # 이렇게 사용하는걸 적극 권장한다.\n",
    "    \n",
    "    # estimator 내부에서 생성되는 로그를 보기위한 것, jupyter에서 실행하면 바로나오더라.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)  \n",
    "    \n",
    "    estimate = tf.estimator.Estimator(model_fn)\n",
    "    estimate.train(input_fn, steps=1000)\n",
    "    estimate.evaluate(input_fn,steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6767985820770264, accuracy: (0.0, 0.5)\n",
      "step: 1, loss: 0.6123156547546387, accuracy: (0.5, 0.5)\n",
      "step: 2, loss: 0.5722694396972656, accuracy: (0.5, 0.5)\n",
      "step: 3, loss: 0.5380921959877014, accuracy: (0.5, 0.5)\n",
      "step: 4, loss: 0.5072094202041626, accuracy: (0.5, 0.5)\n",
      "step: 5, loss: 0.4885789453983307, accuracy: (0.5, 0.5)\n",
      "step: 6, loss: 0.4722108840942383, accuracy: (0.5, 0.5)\n",
      "step: 7, loss: 0.4563263952732086, accuracy: (0.5, 0.5)\n",
      "step: 8, loss: 0.447798490524292, accuracy: (0.5, 0.5)\n",
      "step: 9, loss: 0.4342973828315735, accuracy: (0.5, 0.5)\n",
      "step: 10, loss: 0.4226199686527252, accuracy: (0.5, 0.54545456)\n",
      "step: 11, loss: 0.41301020979881287, accuracy: (0.54545456, 0.58333331)\n",
      "step: 12, loss: 0.40245747566223145, accuracy: (0.58333331, 0.61538464)\n",
      "step: 13, loss: 0.397083580493927, accuracy: (0.61538464, 0.64285713)\n",
      "step: 14, loss: 0.38378798961639404, accuracy: (0.64285713, 0.66666669)\n",
      "step: 15, loss: 0.3729310631752014, accuracy: (0.66666669, 0.6875)\n",
      "step: 16, loss: 0.36388373374938965, accuracy: (0.6875, 0.70588237)\n",
      "step: 17, loss: 0.35246795415878296, accuracy: (0.70588237, 0.72222221)\n",
      "step: 18, loss: 0.3443548083305359, accuracy: (0.72222221, 0.7368421)\n",
      "step: 19, loss: 0.3324151933193207, accuracy: (0.7368421, 0.75)\n",
      "step: 20, loss: 0.32524943351745605, accuracy: (0.75, 0.76190478)\n",
      "step: 21, loss: 0.3137925863265991, accuracy: (0.76190478, 0.77272725)\n",
      "step: 22, loss: 0.3055640757083893, accuracy: (0.77272725, 0.78260869)\n",
      "step: 23, loss: 0.2953526973724365, accuracy: (0.78260869, 0.79166669)\n",
      "step: 24, loss: 0.2862090468406677, accuracy: (0.79166669, 0.80000001)\n",
      "step: 25, loss: 0.2772291898727417, accuracy: (0.80000001, 0.80769229)\n",
      "step: 26, loss: 0.2679961919784546, accuracy: (0.80769229, 0.81481481)\n",
      "step: 27, loss: 0.2616335153579712, accuracy: (0.81481481, 0.8214286)\n",
      "step: 28, loss: 0.25108611583709717, accuracy: (0.8214286, 0.82758623)\n",
      "step: 29, loss: 0.24105770885944366, accuracy: (0.82758623, 0.83333331)\n",
      "step: 30, loss: 0.23278221487998962, accuracy: (0.83333331, 0.83870965)\n",
      "step: 31, loss: 0.22503761947155, accuracy: (0.83870965, 0.84375)\n",
      "step: 32, loss: 0.21555745601654053, accuracy: (0.84375, 0.84848487)\n",
      "step: 33, loss: 0.2087964564561844, accuracy: (0.84848487, 0.85294116)\n",
      "step: 34, loss: 0.19980597496032715, accuracy: (0.85294116, 0.85714287)\n",
      "step: 35, loss: 0.19271321594715118, accuracy: (0.85714287, 0.8611111)\n",
      "step: 36, loss: 0.18489524722099304, accuracy: (0.8611111, 0.86486489)\n",
      "step: 37, loss: 0.1775726079940796, accuracy: (0.86486489, 0.86842108)\n",
      "step: 38, loss: 0.17076244950294495, accuracy: (0.86842108, 0.87179488)\n",
      "step: 39, loss: 0.16434970498085022, accuracy: (0.87179488, 0.875)\n",
      "step: 40, loss: 0.1587885320186615, accuracy: (0.875, 0.87804878)\n",
      "step: 41, loss: 0.15164804458618164, accuracy: (0.87804878, 0.88095236)\n",
      "step: 42, loss: 0.14485342800617218, accuracy: (0.88095236, 0.88372093)\n",
      "step: 43, loss: 0.13977977633476257, accuracy: (0.88372093, 0.88636363)\n",
      "step: 44, loss: 0.1335274875164032, accuracy: (0.88636363, 0.8888889)\n",
      "step: 45, loss: 0.1286146640777588, accuracy: (0.8888889, 0.89130437)\n",
      "step: 46, loss: 0.12307287752628326, accuracy: (0.89130437, 0.89361703)\n",
      "step: 47, loss: 0.11880906671285629, accuracy: (0.89361703, 0.89583331)\n",
      "step: 48, loss: 0.1143876314163208, accuracy: (0.89583331, 0.89795917)\n",
      "step: 49, loss: 0.10942719876766205, accuracy: (0.89795917, 0.89999998)\n",
      "step: 50, loss: 0.10515116155147552, accuracy: (0.89999998, 0.90196079)\n",
      "step: 51, loss: 0.10125912725925446, accuracy: (0.90196079, 0.90384614)\n",
      "step: 52, loss: 0.09719717502593994, accuracy: (0.90384614, 0.90566039)\n",
      "step: 53, loss: 0.09366397559642792, accuracy: (0.90566039, 0.9074074)\n",
      "step: 54, loss: 0.09044641256332397, accuracy: (0.9074074, 0.90909094)\n",
      "step: 55, loss: 0.0873338133096695, accuracy: (0.90909094, 0.91071427)\n",
      "step: 56, loss: 0.08380597084760666, accuracy: (0.91071427, 0.91228068)\n",
      "step: 57, loss: 0.08072830736637115, accuracy: (0.91228068, 0.91379309)\n",
      "step: 58, loss: 0.07780236005783081, accuracy: (0.91379309, 0.91525424)\n",
      "step: 59, loss: 0.07539094984531403, accuracy: (0.91525424, 0.91666669)\n",
      "step: 60, loss: 0.07284025847911835, accuracy: (0.91666669, 0.91803277)\n",
      "step: 61, loss: 0.07005372643470764, accuracy: (0.91803277, 0.91935486)\n",
      "step: 62, loss: 0.06783751398324966, accuracy: (0.91935486, 0.92063493)\n",
      "step: 63, loss: 0.06539573520421982, accuracy: (0.92063493, 0.921875)\n",
      "step: 64, loss: 0.06339935958385468, accuracy: (0.921875, 0.92307693)\n",
      "step: 65, loss: 0.061406075954437256, accuracy: (0.92307693, 0.92424244)\n",
      "step: 66, loss: 0.059437789022922516, accuracy: (0.92424244, 0.92537314)\n",
      "step: 67, loss: 0.057460106909275055, accuracy: (0.92537314, 0.92647058)\n",
      "step: 68, loss: 0.05596628040075302, accuracy: (0.92647058, 0.92753625)\n",
      "step: 69, loss: 0.05418039485812187, accuracy: (0.92753625, 0.9285714)\n",
      "step: 70, loss: 0.05248720571398735, accuracy: (0.9285714, 0.92957747)\n",
      "step: 71, loss: 0.0508999340236187, accuracy: (0.92957747, 0.93055558)\n",
      "step: 72, loss: 0.04954633116722107, accuracy: (0.93055558, 0.93150687)\n",
      "step: 73, loss: 0.048128969967365265, accuracy: (0.93150687, 0.93243241)\n",
      "step: 74, loss: 0.04664096236228943, accuracy: (0.93243241, 0.93333334)\n",
      "step: 75, loss: 0.045354828238487244, accuracy: (0.93333334, 0.93421054)\n",
      "step: 76, loss: 0.0442197322845459, accuracy: (0.93421054, 0.93506491)\n",
      "step: 77, loss: 0.043007537722587585, accuracy: (0.93506491, 0.93589741)\n",
      "step: 78, loss: 0.04175368323922157, accuracy: (0.93589741, 0.93670887)\n",
      "step: 79, loss: 0.040658675134181976, accuracy: (0.93670887, 0.9375)\n",
      "step: 80, loss: 0.03968097269535065, accuracy: (0.9375, 0.93827158)\n",
      "step: 81, loss: 0.038670316338539124, accuracy: (0.93827158, 0.93902439)\n",
      "step: 82, loss: 0.03769862651824951, accuracy: (0.93902439, 0.93975902)\n",
      "step: 83, loss: 0.036848217248916626, accuracy: (0.93975902, 0.94047618)\n",
      "step: 84, loss: 0.035836733877658844, accuracy: (0.94047618, 0.94117647)\n",
      "step: 85, loss: 0.035110969096422195, accuracy: (0.94117647, 0.94186044)\n",
      "step: 86, loss: 0.03414512798190117, accuracy: (0.94186044, 0.94252872)\n",
      "step: 87, loss: 0.03339168056845665, accuracy: (0.94252872, 0.94318181)\n",
      "step: 88, loss: 0.0325971283018589, accuracy: (0.94318181, 0.94382024)\n",
      "step: 89, loss: 0.031858716160058975, accuracy: (0.94382024, 0.94444442)\n",
      "step: 90, loss: 0.03125201165676117, accuracy: (0.94505495, 0.94505495)\n",
      "step: 91, loss: 0.030494239181280136, accuracy: (0.94505495, 0.94565219)\n",
      "step: 92, loss: 0.02982906624674797, accuracy: (0.94565219, 0.94623655)\n",
      "step: 93, loss: 0.029188405722379684, accuracy: (0.94623655, 0.94680852)\n",
      "step: 94, loss: 0.028544355183839798, accuracy: (0.94680852, 0.94736844)\n",
      "step: 95, loss: 0.027994215488433838, accuracy: (0.94736844, 0.94791669)\n",
      "step: 96, loss: 0.027333980426192284, accuracy: (0.94791669, 0.94845361)\n",
      "step: 97, loss: 0.026800986379384995, accuracy: (0.94845361, 0.94897962)\n",
      "step: 98, loss: 0.026217138394713402, accuracy: (0.94897962, 0.94949496)\n",
      "step: 99, loss: 0.025733623653650284, accuracy: (0.94949496, 0.94999999)\n",
      "step: 100, loss: 0.025191236287355423, accuracy: (0.94999999, 0.95049506)\n",
      "step: 101, loss: 0.024717791005969048, accuracy: (0.95049506, 0.95098037)\n",
      "step: 102, loss: 0.024223698303103447, accuracy: (0.95098037, 0.95145631)\n",
      "step: 103, loss: 0.02376420423388481, accuracy: (0.95145631, 0.95192307)\n",
      "step: 104, loss: 0.02332685887813568, accuracy: (0.95192307, 0.95238096)\n",
      "step: 105, loss: 0.02290954254567623, accuracy: (0.95238096, 0.9528302)\n",
      "step: 106, loss: 0.022464022040367126, accuracy: (0.9528302, 0.95327103)\n",
      "step: 107, loss: 0.022051453590393066, accuracy: (0.95327103, 0.9537037)\n",
      "step: 108, loss: 0.021676601842045784, accuracy: (0.9537037, 0.95412844)\n",
      "step: 109, loss: 0.021278638392686844, accuracy: (0.95412844, 0.95454544)\n",
      "step: 110, loss: 0.020896058529615402, accuracy: (0.95454544, 0.95495498)\n",
      "step: 111, loss: 0.020532434806227684, accuracy: (0.95495498, 0.95535713)\n",
      "step: 112, loss: 0.020172256976366043, accuracy: (0.95535713, 0.95575219)\n",
      "step: 113, loss: 0.01983661949634552, accuracy: (0.95575219, 0.95614034)\n",
      "step: 114, loss: 0.019486723467707634, accuracy: (0.95614034, 0.95652175)\n",
      "step: 115, loss: 0.019168386235833168, accuracy: (0.95652175, 0.95689654)\n",
      "step: 116, loss: 0.01884307526051998, accuracy: (0.95689654, 0.95726496)\n",
      "step: 117, loss: 0.018549220636487007, accuracy: (0.95726496, 0.95762712)\n",
      "step: 118, loss: 0.018221821635961533, accuracy: (0.95762712, 0.9579832)\n",
      "step: 119, loss: 0.017948634922504425, accuracy: (0.9579832, 0.95833331)\n",
      "step: 120, loss: 0.017642300575971603, accuracy: (0.95833331, 0.95867771)\n",
      "step: 121, loss: 0.017382105812430382, accuracy: (0.95867771, 0.95901638)\n",
      "step: 122, loss: 0.017095103859901428, accuracy: (0.95901638, 0.95934957)\n",
      "step: 123, loss: 0.0168300773948431, accuracy: (0.95934957, 0.9596774)\n",
      "step: 124, loss: 0.016583066433668137, accuracy: (0.9596774, 0.95999998)\n",
      "step: 125, loss: 0.01632523536682129, accuracy: (0.95999998, 0.96031743)\n",
      "step: 126, loss: 0.01609182171523571, accuracy: (0.96031743, 0.96062994)\n",
      "step: 127, loss: 0.0158330537378788, accuracy: (0.96062994, 0.9609375)\n",
      "step: 128, loss: 0.015613600611686707, accuracy: (0.9609375, 0.96124029)\n",
      "step: 129, loss: 0.015373781323432922, accuracy: (0.96124029, 0.96153843)\n",
      "step: 130, loss: 0.015161106362938881, accuracy: (0.96153843, 0.96183205)\n",
      "step: 131, loss: 0.0149367181584239, accuracy: (0.96183205, 0.96212119)\n",
      "step: 132, loss: 0.014736732468008995, accuracy: (0.96212119, 0.96240604)\n",
      "step: 133, loss: 0.014512315392494202, accuracy: (0.96240604, 0.96268654)\n",
      "step: 134, loss: 0.01432432234287262, accuracy: (0.96268654, 0.96296299)\n",
      "step: 135, loss: 0.01411446463316679, accuracy: (0.96296299, 0.96323532)\n",
      "step: 136, loss: 0.013934342190623283, accuracy: (0.96323532, 0.96350366)\n",
      "step: 137, loss: 0.013729316182434559, accuracy: (0.96350366, 0.96376812)\n",
      "step: 138, loss: 0.013548457995057106, accuracy: (0.96376812, 0.96402878)\n",
      "step: 139, loss: 0.013374373316764832, accuracy: (0.96402878, 0.96428573)\n",
      "step: 140, loss: 0.01319255493581295, accuracy: (0.96428573, 0.96453899)\n",
      "step: 141, loss: 0.013025561347603798, accuracy: (0.96453899, 0.96478873)\n",
      "step: 142, loss: 0.012849366292357445, accuracy: (0.96478873, 0.96503496)\n",
      "step: 143, loss: 0.01268815714865923, accuracy: (0.96503496, 0.96527779)\n",
      "step: 144, loss: 0.012523038312792778, accuracy: (0.96527779, 0.96551722)\n",
      "step: 145, loss: 0.012363092973828316, accuracy: (0.96551722, 0.96575344)\n",
      "step: 146, loss: 0.012212837114930153, accuracy: (0.96575344, 0.96598637)\n",
      "step: 147, loss: 0.012058092281222343, accuracy: (0.96598637, 0.96621621)\n",
      "step: 148, loss: 0.011909015476703644, accuracy: (0.96621621, 0.96644294)\n",
      "step: 149, loss: 0.011759119108319283, accuracy: (0.96644294, 0.96666664)\n",
      "step: 150, loss: 0.01162264496088028, accuracy: (0.96666664, 0.96688741)\n",
      "step: 151, loss: 0.011479586362838745, accuracy: (0.96688741, 0.96710527)\n",
      "step: 152, loss: 0.011343609541654587, accuracy: (0.96710527, 0.96732026)\n",
      "step: 153, loss: 0.011204473674297333, accuracy: (0.96732026, 0.96753246)\n",
      "step: 154, loss: 0.01107867993414402, accuracy: (0.96753246, 0.96774191)\n",
      "step: 155, loss: 0.010944230481982231, accuracy: (0.96774191, 0.96794873)\n",
      "step: 156, loss: 0.010822303593158722, accuracy: (0.96794873, 0.96815288)\n",
      "step: 157, loss: 0.010691659525036812, accuracy: (0.96815288, 0.9683544)\n",
      "step: 158, loss: 0.010566161945462227, accuracy: (0.9683544, 0.96855348)\n",
      "step: 159, loss: 0.010460523888468742, accuracy: (0.96855348, 0.96875)\n",
      "step: 160, loss: 0.010330615565180779, accuracy: (0.96875, 0.96894407)\n",
      "step: 161, loss: 0.010226799175143242, accuracy: (0.96894407, 0.96913582)\n",
      "step: 162, loss: 0.010098867118358612, accuracy: (0.96913582, 0.96932513)\n",
      "step: 163, loss: 0.010004262439906597, accuracy: (0.96932513, 0.96951222)\n",
      "step: 164, loss: 0.009880590252578259, accuracy: (0.96951222, 0.969697)\n",
      "step: 165, loss: 0.009779125452041626, accuracy: (0.969697, 0.96987951)\n",
      "step: 166, loss: 0.009669234976172447, accuracy: (0.96987951, 0.97005987)\n",
      "step: 167, loss: 0.009576855227351189, accuracy: (0.97005987, 0.97023809)\n",
      "step: 168, loss: 0.00946702342480421, accuracy: (0.97023809, 0.97041422)\n",
      "step: 169, loss: 0.009373631328344345, accuracy: (0.97041422, 0.97058821)\n",
      "step: 170, loss: 0.009271136485040188, accuracy: (0.97058821, 0.97076023)\n",
      "step: 171, loss: 0.009176612831652164, accuracy: (0.97076023, 0.97093022)\n",
      "step: 172, loss: 0.009083720855414867, accuracy: (0.97093022, 0.97109824)\n",
      "step: 173, loss: 0.00899133924394846, accuracy: (0.97109824, 0.97126436)\n",
      "step: 174, loss: 0.008898386731743813, accuracy: (0.97126436, 0.97142857)\n",
      "step: 175, loss: 0.008807359263300896, accuracy: (0.97142857, 0.97159094)\n",
      "step: 176, loss: 0.008723406121134758, accuracy: (0.97159094, 0.97175139)\n",
      "step: 177, loss: 0.008633268997073174, accuracy: (0.97175139, 0.97191012)\n",
      "step: 178, loss: 0.008550622500479221, accuracy: (0.97191012, 0.97206706)\n",
      "step: 179, loss: 0.008462617173790932, accuracy: (0.97206706, 0.97222221)\n",
      "step: 180, loss: 0.008377877064049244, accuracy: (0.97222221, 0.97237569)\n",
      "step: 181, loss: 0.008299721404910088, accuracy: (0.97237569, 0.97252744)\n",
      "step: 182, loss: 0.008217183873057365, accuracy: (0.97252744, 0.97267759)\n",
      "step: 183, loss: 0.008149842731654644, accuracy: (0.97267759, 0.97282606)\n",
      "step: 184, loss: 0.00806142296642065, accuracy: (0.97282606, 0.97297299)\n",
      "step: 185, loss: 0.007993949577212334, accuracy: (0.97297299, 0.97311831)\n",
      "step: 186, loss: 0.007912966422736645, accuracy: (0.97311831, 0.97326201)\n",
      "step: 187, loss: 0.007845679298043251, accuracy: (0.97326201, 0.97340423)\n",
      "step: 188, loss: 0.007766813971102238, accuracy: (0.97340423, 0.97354496)\n",
      "step: 189, loss: 0.007699230220168829, accuracy: (0.97368419, 0.97368419)\n",
      "step: 190, loss: 0.007627473212778568, accuracy: (0.97368419, 0.973822)\n",
      "step: 191, loss: 0.007560196332633495, accuracy: (0.973822, 0.97395831)\n",
      "step: 192, loss: 0.007490138523280621, accuracy: (0.97395831, 0.97409326)\n",
      "step: 193, loss: 0.007422897964715958, accuracy: (0.97409326, 0.97422683)\n",
      "step: 194, loss: 0.007359236478805542, accuracy: (0.97422683, 0.97435898)\n",
      "step: 195, loss: 0.0072927214205265045, accuracy: (0.97435898, 0.97448981)\n",
      "step: 196, loss: 0.007229664362967014, accuracy: (0.97448981, 0.97461927)\n",
      "step: 197, loss: 0.007163195870816708, accuracy: (0.97461927, 0.97474748)\n",
      "step: 198, loss: 0.0071064806543290615, accuracy: (0.97474748, 0.97487438)\n",
      "step: 199, loss: 0.0070400601252913475, accuracy: (0.97487438, 0.97500002)\n",
      "step: 200, loss: 0.0069844676181674, accuracy: (0.97500002, 0.97512436)\n",
      "step: 201, loss: 0.006919653620570898, accuracy: (0.97512436, 0.9752475)\n",
      "step: 202, loss: 0.0068613155744969845, accuracy: (0.9752475, 0.97536945)\n",
      "step: 203, loss: 0.006803624331951141, accuracy: (0.97536945, 0.97549021)\n",
      "step: 204, loss: 0.0067475223913788795, accuracy: (0.97549021, 0.97560978)\n",
      "step: 205, loss: 0.006695246789604425, accuracy: (0.97560978, 0.97572815)\n",
      "step: 206, loss: 0.006636091507971287, accuracy: (0.97572815, 0.9758454)\n",
      "step: 207, loss: 0.0065834298729896545, accuracy: (0.9758454, 0.97596157)\n",
      "step: 208, loss: 0.006529458798468113, accuracy: (0.97596157, 0.97607654)\n",
      "step: 209, loss: 0.0064763217233121395, accuracy: (0.97607654, 0.97619045)\n",
      "step: 210, loss: 0.00642398651689291, accuracy: (0.97619045, 0.97630334)\n",
      "step: 211, loss: 0.006370489485561848, accuracy: (0.97630334, 0.9764151)\n",
      "step: 212, loss: 0.006323212292045355, accuracy: (0.9764151, 0.97652584)\n",
      "step: 213, loss: 0.006269319448620081, accuracy: (0.97652584, 0.97663552)\n",
      "step: 214, loss: 0.006223265081644058, accuracy: (0.97663552, 0.97674417)\n",
      "step: 215, loss: 0.0061692302115261555, accuracy: (0.97674417, 0.97685188)\n",
      "step: 216, loss: 0.006127861328423023, accuracy: (0.97685188, 0.97695851)\n",
      "step: 217, loss: 0.006074549630284309, accuracy: (0.97695851, 0.97706419)\n",
      "step: 218, loss: 0.0060288249514997005, accuracy: (0.97706419, 0.97716898)\n",
      "step: 219, loss: 0.005981387570500374, accuracy: (0.97716898, 0.97727275)\n",
      "step: 220, loss: 0.005939412862062454, accuracy: (0.97727275, 0.97737557)\n",
      "step: 221, loss: 0.005891887471079826, accuracy: (0.97737557, 0.97747749)\n",
      "step: 222, loss: 0.005849468056112528, accuracy: (0.97747749, 0.97757846)\n",
      "step: 223, loss: 0.00580355990678072, accuracy: (0.97757846, 0.9776786)\n",
      "step: 224, loss: 0.005760403349995613, accuracy: (0.9776786, 0.97777778)\n",
      "step: 225, loss: 0.005718707572668791, accuracy: (0.97777778, 0.97787613)\n",
      "step: 226, loss: 0.005675115622580051, accuracy: (0.97787613, 0.97797358)\n",
      "step: 227, loss: 0.005634644068777561, accuracy: (0.97797358, 0.9780702)\n",
      "step: 228, loss: 0.005590418353676796, accuracy: (0.9780702, 0.97816592)\n",
      "step: 229, loss: 0.005554111208766699, accuracy: (0.97816592, 0.97826087)\n",
      "step: 230, loss: 0.005509709008038044, accuracy: (0.97826087, 0.97835499)\n",
      "step: 231, loss: 0.005470858421176672, accuracy: (0.97835499, 0.97844827)\n",
      "step: 232, loss: 0.005430649500340223, accuracy: (0.97844827, 0.97854078)\n",
      "step: 233, loss: 0.005393076688051224, accuracy: (0.97854078, 0.97863245)\n",
      "step: 234, loss: 0.005354581866413355, accuracy: (0.97863245, 0.97872341)\n",
      "step: 235, loss: 0.005319230258464813, accuracy: (0.97872341, 0.97881359)\n",
      "step: 236, loss: 0.0052796462550759315, accuracy: (0.97881359, 0.97890294)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 237, loss: 0.005243467632681131, accuracy: (0.97890294, 0.97899157)\n",
      "step: 238, loss: 0.00520747248083353, accuracy: (0.97899157, 0.97907948)\n",
      "step: 239, loss: 0.005170801654458046, accuracy: (0.97907948, 0.97916669)\n",
      "step: 240, loss: 0.005135903134942055, accuracy: (0.97916669, 0.97925311)\n",
      "step: 241, loss: 0.0050988816656172276, accuracy: (0.97925311, 0.97933882)\n",
      "step: 242, loss: 0.005064092110842466, accuracy: (0.97933882, 0.97942388)\n",
      "step: 243, loss: 0.0050299568101763725, accuracy: (0.97942388, 0.97950822)\n",
      "step: 244, loss: 0.004995984025299549, accuracy: (0.97950822, 0.97959185)\n",
      "step: 245, loss: 0.004962156526744366, accuracy: (0.97959185, 0.97967482)\n",
      "step: 246, loss: 0.004929127637296915, accuracy: (0.97967482, 0.97975707)\n",
      "step: 247, loss: 0.004896440543234348, accuracy: (0.97975707, 0.97983873)\n",
      "step: 248, loss: 0.004865655675530434, accuracy: (0.97983873, 0.97991967)\n",
      "step: 249, loss: 0.004832994192838669, accuracy: (0.97991967, 0.98000002)\n",
      "step: 250, loss: 0.004801517352461815, accuracy: (0.98000002, 0.98007971)\n",
      "step: 251, loss: 0.0047700656577944756, accuracy: (0.98007971, 0.98015875)\n",
      "step: 252, loss: 0.0047383541241288185, accuracy: (0.98015875, 0.98023713)\n",
      "step: 253, loss: 0.004707010462880135, accuracy: (0.98023713, 0.98031497)\n",
      "step: 254, loss: 0.004677075892686844, accuracy: (0.98031497, 0.98039216)\n",
      "step: 255, loss: 0.004646712448447943, accuracy: (0.98039216, 0.98046875)\n",
      "step: 256, loss: 0.004617644473910332, accuracy: (0.98046875, 0.98054475)\n",
      "step: 257, loss: 0.00458779651671648, accuracy: (0.98054475, 0.98062015)\n",
      "step: 258, loss: 0.004561822861433029, accuracy: (0.98062015, 0.98069501)\n",
      "step: 259, loss: 0.004530463367700577, accuracy: (0.98069501, 0.98076922)\n",
      "step: 260, loss: 0.004503798671066761, accuracy: (0.98076922, 0.98084289)\n",
      "step: 261, loss: 0.004474905785173178, accuracy: (0.98084289, 0.98091602)\n",
      "step: 262, loss: 0.004447544924914837, accuracy: (0.98091602, 0.98098862)\n",
      "step: 263, loss: 0.004419783595949411, accuracy: (0.98098862, 0.98106062)\n",
      "step: 264, loss: 0.004391986411064863, accuracy: (0.98106062, 0.98113209)\n",
      "step: 265, loss: 0.004366752225905657, accuracy: (0.98113209, 0.98120302)\n",
      "step: 266, loss: 0.004338117316365242, accuracy: (0.98120302, 0.98127341)\n",
      "step: 267, loss: 0.004313800483942032, accuracy: (0.98127341, 0.98134327)\n",
      "step: 268, loss: 0.00428578769788146, accuracy: (0.98134327, 0.98141265)\n",
      "step: 269, loss: 0.004262184724211693, accuracy: (0.98141265, 0.98148149)\n",
      "step: 270, loss: 0.004234188701957464, accuracy: (0.98148149, 0.9815498)\n",
      "step: 271, loss: 0.0042095063254237175, accuracy: (0.9815498, 0.98161763)\n",
      "step: 272, loss: 0.00418453011661768, accuracy: (0.98161763, 0.98168498)\n",
      "step: 273, loss: 0.004160829819738865, accuracy: (0.98168498, 0.9817518)\n",
      "step: 274, loss: 0.004136238247156143, accuracy: (0.9817518, 0.9818182)\n",
      "step: 275, loss: 0.004111734218895435, accuracy: (0.9818182, 0.98188406)\n",
      "step: 276, loss: 0.004088078625500202, accuracy: (0.98188406, 0.98194945)\n",
      "step: 277, loss: 0.004063885658979416, accuracy: (0.98194945, 0.98201442)\n",
      "step: 278, loss: 0.004041018895804882, accuracy: (0.98201442, 0.98207885)\n",
      "step: 279, loss: 0.004016057122498751, accuracy: (0.98207885, 0.98214287)\n",
      "step: 280, loss: 0.003995460923761129, accuracy: (0.98214287, 0.9822064)\n",
      "step: 281, loss: 0.003970102872699499, accuracy: (0.9822064, 0.98226953)\n",
      "step: 282, loss: 0.003950151149183512, accuracy: (0.98226953, 0.98233217)\n",
      "step: 283, loss: 0.003925510682165623, accuracy: (0.98233217, 0.98239434)\n",
      "step: 284, loss: 0.0039047207683324814, accuracy: (0.98239434, 0.98245615)\n",
      "step: 285, loss: 0.003882176475599408, accuracy: (0.98245615, 0.98251748)\n",
      "step: 286, loss: 0.0038609537295997143, accuracy: (0.98251748, 0.9825784)\n",
      "step: 287, loss: 0.0038390723057091236, accuracy: (0.9825784, 0.9826389)\n",
      "step: 288, loss: 0.003817283781245351, accuracy: (0.9826389, 0.98269898)\n",
      "step: 289, loss: 0.003797421930357814, accuracy: (0.98269898, 0.98275864)\n",
      "step: 290, loss: 0.0037753030192106962, accuracy: (0.98275864, 0.98281789)\n",
      "step: 291, loss: 0.0037558339536190033, accuracy: (0.98281789, 0.98287672)\n",
      "step: 292, loss: 0.003733281511813402, accuracy: (0.98287672, 0.98293513)\n",
      "step: 293, loss: 0.003715336322784424, accuracy: (0.98293513, 0.98299319)\n",
      "step: 294, loss: 0.003693657461553812, accuracy: (0.98299319, 0.98305082)\n",
      "step: 295, loss: 0.0036751539446413517, accuracy: (0.98305082, 0.9831081)\n",
      "step: 296, loss: 0.0036540073342621326, accuracy: (0.9831081, 0.98316497)\n",
      "step: 297, loss: 0.0036349412985146046, accuracy: (0.98316497, 0.98322147)\n",
      "step: 298, loss: 0.0036159143783152103, accuracy: (0.98322147, 0.98327762)\n",
      "step: 299, loss: 0.0035962513647973537, accuracy: (0.98327762, 0.98333335)\n",
      "step: 300, loss: 0.003577662631869316, accuracy: (0.98333335, 0.98338872)\n",
      "step: 301, loss: 0.003558063879609108, accuracy: (0.98338872, 0.98344374)\n",
      "step: 302, loss: 0.0035403738729655743, accuracy: (0.98344374, 0.98349833)\n",
      "step: 303, loss: 0.0035204500891268253, accuracy: (0.98349833, 0.98355263)\n",
      "step: 304, loss: 0.0035025004763156176, accuracy: (0.98355263, 0.98360658)\n",
      "step: 305, loss: 0.0034840148873627186, accuracy: (0.98360658, 0.9836601)\n",
      "step: 306, loss: 0.003467076225206256, accuracy: (0.9836601, 0.98371333)\n",
      "step: 307, loss: 0.003449052106589079, accuracy: (0.98371333, 0.98376626)\n",
      "step: 308, loss: 0.0034315239172428846, accuracy: (0.98376626, 0.98381877)\n",
      "step: 309, loss: 0.0034137428738176823, accuracy: (0.98381877, 0.98387098)\n",
      "step: 310, loss: 0.0033964035101234913, accuracy: (0.98387098, 0.98392284)\n",
      "step: 311, loss: 0.0033793398179113865, accuracy: (0.98392284, 0.98397434)\n",
      "step: 312, loss: 0.003361503826454282, accuracy: (0.98397434, 0.98402554)\n",
      "step: 313, loss: 0.0033459465485066175, accuracy: (0.98402554, 0.98407644)\n",
      "step: 314, loss: 0.0033275247551500797, accuracy: (0.98407644, 0.98412699)\n",
      "step: 315, loss: 0.003312624292448163, accuracy: (0.98412699, 0.98417723)\n",
      "step: 316, loss: 0.0032945056445896626, accuracy: (0.98417723, 0.98422712)\n",
      "step: 317, loss: 0.0032786645460873842, accuracy: (0.98422712, 0.98427671)\n",
      "step: 318, loss: 0.0032621952705085278, accuracy: (0.98427671, 0.984326)\n",
      "step: 319, loss: 0.003246889915317297, accuracy: (0.984326, 0.984375)\n",
      "step: 320, loss: 0.003230723086744547, accuracy: (0.984375, 0.9844237)\n",
      "step: 321, loss: 0.003214604454115033, accuracy: (0.9844237, 0.98447204)\n",
      "step: 322, loss: 0.0031998497433960438, accuracy: (0.98447204, 0.98452014)\n",
      "step: 323, loss: 0.0031831569503992796, accuracy: (0.98452014, 0.98456788)\n",
      "step: 324, loss: 0.0031690639443695545, accuracy: (0.98456788, 0.98461539)\n",
      "step: 325, loss: 0.0031525727827101946, accuracy: (0.98461539, 0.98466259)\n",
      "step: 326, loss: 0.003137740073725581, accuracy: (0.98466259, 0.9847095)\n",
      "step: 327, loss: 0.003122270107269287, accuracy: (0.9847095, 0.98475611)\n",
      "step: 328, loss: 0.003108565229922533, accuracy: (0.98475611, 0.98480242)\n",
      "step: 329, loss: 0.0030931062065064907, accuracy: (0.98480242, 0.9848485)\n",
      "step: 330, loss: 0.0030790320597589016, accuracy: (0.9848485, 0.98489428)\n",
      "step: 331, loss: 0.0030640882905572653, accuracy: (0.98489428, 0.98493975)\n",
      "step: 332, loss: 0.0030494341626763344, accuracy: (0.98493975, 0.98498499)\n",
      "step: 333, loss: 0.003036028239876032, accuracy: (0.98498499, 0.98502994)\n",
      "step: 334, loss: 0.0030208881944417953, accuracy: (0.98502994, 0.98507464)\n",
      "step: 335, loss: 0.00300778029486537, accuracy: (0.98507464, 0.98511904)\n",
      "step: 336, loss: 0.0029927375726401806, accuracy: (0.98511904, 0.98516321)\n",
      "step: 337, loss: 0.002979172393679619, accuracy: (0.98516321, 0.98520708)\n",
      "step: 338, loss: 0.002965501043945551, accuracy: (0.98520708, 0.98525071)\n",
      "step: 339, loss: 0.0029527065344154835, accuracy: (0.98525071, 0.9852941)\n",
      "step: 340, loss: 0.002938461024314165, accuracy: (0.9852941, 0.98533726)\n",
      "step: 341, loss: 0.002925216220319271, accuracy: (0.98533726, 0.98538011)\n",
      "step: 342, loss: 0.0029124016873538494, accuracy: (0.98538011, 0.98542273)\n",
      "step: 343, loss: 0.0028986306861042976, accuracy: (0.98542273, 0.98546511)\n",
      "step: 344, loss: 0.0028861770406365395, accuracy: (0.98546511, 0.98550725)\n",
      "step: 345, loss: 0.002872360870242119, accuracy: (0.98550725, 0.98554915)\n",
      "step: 346, loss: 0.002860546112060547, accuracy: (0.98554915, 0.98559076)\n",
      "step: 347, loss: 0.0028467022348195314, accuracy: (0.98559076, 0.98563218)\n",
      "step: 348, loss: 0.0028342283330857754, accuracy: (0.98563218, 0.98567337)\n",
      "step: 349, loss: 0.002821641508489847, accuracy: (0.98567337, 0.98571426)\n",
      "step: 350, loss: 0.00280991243198514, accuracy: (0.98571426, 0.98575497)\n",
      "step: 351, loss: 0.002796982415020466, accuracy: (0.98575497, 0.98579544)\n",
      "step: 352, loss: 0.0027847536839544773, accuracy: (0.98579544, 0.98583567)\n",
      "step: 353, loss: 0.0027729440480470657, accuracy: (0.98583567, 0.98587573)\n",
      "step: 354, loss: 0.002760267350822687, accuracy: (0.98587573, 0.98591548)\n",
      "step: 355, loss: 0.0027489489875733852, accuracy: (0.98591548, 0.98595506)\n",
      "step: 356, loss: 0.002736259251832962, accuracy: (0.98595506, 0.9859944)\n",
      "step: 357, loss: 0.0027245436795055866, accuracy: (0.9859944, 0.9860335)\n",
      "step: 358, loss: 0.002712637186050415, accuracy: (0.9860335, 0.98607242)\n",
      "step: 359, loss: 0.0027012289501726627, accuracy: (0.98607242, 0.9861111)\n",
      "step: 360, loss: 0.002689668443053961, accuracy: (0.9861111, 0.98614961)\n",
      "step: 361, loss: 0.0026784506626427174, accuracy: (0.98614961, 0.98618782)\n",
      "step: 362, loss: 0.002667288528755307, accuracy: (0.98618782, 0.9862259)\n",
      "step: 363, loss: 0.00265559833496809, accuracy: (0.9862259, 0.98626375)\n",
      "step: 364, loss: 0.0026448694989085197, accuracy: (0.98626375, 0.98630136)\n",
      "step: 365, loss: 0.002633105730637908, accuracy: (0.98630136, 0.98633879)\n",
      "step: 366, loss: 0.0026227363850921392, accuracy: (0.98633879, 0.98637605)\n",
      "step: 367, loss: 0.002610903000459075, accuracy: (0.98637605, 0.98641306)\n",
      "step: 368, loss: 0.0026001790538430214, accuracy: (0.98641306, 0.98644984)\n",
      "step: 369, loss: 0.002589453011751175, accuracy: (0.98644984, 0.98648649)\n",
      "step: 370, loss: 0.002579323947429657, accuracy: (0.98648649, 0.98652291)\n",
      "step: 371, loss: 0.002568113151937723, accuracy: (0.98652291, 0.98655915)\n",
      "step: 372, loss: 0.0025577303022146225, accuracy: (0.98655915, 0.98659515)\n",
      "step: 373, loss: 0.002547513460740447, accuracy: (0.98659515, 0.98663104)\n",
      "step: 374, loss: 0.002536637242883444, accuracy: (0.98663104, 0.98666668)\n",
      "step: 375, loss: 0.0025267633609473705, accuracy: (0.98666668, 0.98670214)\n",
      "step: 376, loss: 0.002515847561880946, accuracy: (0.98670214, 0.98673743)\n",
      "step: 377, loss: 0.0025064200162887573, accuracy: (0.98673743, 0.98677248)\n",
      "step: 378, loss: 0.0024953559041023254, accuracy: (0.98677248, 0.98680741)\n",
      "step: 379, loss: 0.0024861879646778107, accuracy: (0.98680741, 0.9868421)\n",
      "step: 380, loss: 0.0024756151251494884, accuracy: (0.9868421, 0.98687667)\n",
      "step: 381, loss: 0.0024661822244524956, accuracy: (0.98687667, 0.986911)\n",
      "step: 382, loss: 0.0024559672456234694, accuracy: (0.986911, 0.98694515)\n",
      "step: 383, loss: 0.0024460777640342712, accuracy: (0.98694515, 0.98697919)\n",
      "step: 384, loss: 0.0024368343874812126, accuracy: (0.98697919, 0.98701298)\n",
      "step: 385, loss: 0.0024266317486763, accuracy: (0.98701298, 0.98704666)\n",
      "step: 386, loss: 0.002417690586298704, accuracy: (0.98704666, 0.9870801)\n",
      "step: 387, loss: 0.0024074034299701452, accuracy: (0.9870801, 0.98711342)\n",
      "step: 388, loss: 0.0023981602862477303, accuracy: (0.98711342, 0.98714656)\n",
      "step: 389, loss: 0.0023886330891400576, accuracy: (0.98714656, 0.98717946)\n",
      "step: 390, loss: 0.0023799454793334007, accuracy: (0.98717946, 0.9872123)\n",
      "step: 391, loss: 0.0023703041952103376, accuracy: (0.9872123, 0.9872449)\n",
      "step: 392, loss: 0.0023613881785422564, accuracy: (0.9872449, 0.98727733)\n",
      "step: 393, loss: 0.0023521040566265583, accuracy: (0.98727733, 0.98730963)\n",
      "step: 394, loss: 0.0023428460117429495, accuracy: (0.98730963, 0.98734176)\n",
      "step: 395, loss: 0.0023343779612332582, accuracy: (0.98734176, 0.98737371)\n",
      "step: 396, loss: 0.002324718749150634, accuracy: (0.98737371, 0.98740554)\n",
      "step: 397, loss: 0.002316578757017851, accuracy: (0.98740554, 0.98743719)\n",
      "step: 398, loss: 0.002306931186467409, accuracy: (0.98743719, 0.98746866)\n",
      "step: 399, loss: 0.002298567211255431, accuracy: (0.98746866, 0.98750001)\n",
      "step: 400, loss: 0.002289647702127695, accuracy: (0.98750001, 0.98753119)\n",
      "step: 401, loss: 0.002281365916132927, accuracy: (0.98753119, 0.98756218)\n",
      "step: 402, loss: 0.0022726573515683413, accuracy: (0.98756218, 0.98759305)\n",
      "step: 403, loss: 0.0022641511168330908, accuracy: (0.98759305, 0.98762375)\n",
      "step: 404, loss: 0.002255758736282587, accuracy: (0.98762375, 0.98765433)\n",
      "step: 405, loss: 0.0022468343377113342, accuracy: (0.98765433, 0.98768473)\n",
      "step: 406, loss: 0.0022392626851797104, accuracy: (0.98768473, 0.98771501)\n",
      "step: 407, loss: 0.0022301170974969864, accuracy: (0.98771501, 0.98774511)\n",
      "step: 408, loss: 0.0022221661638468504, accuracy: (0.98774511, 0.98777509)\n",
      "step: 409, loss: 0.0022138042841106653, accuracy: (0.98777509, 0.98780489)\n",
      "step: 410, loss: 0.002205989556387067, accuracy: (0.98780489, 0.98783457)\n",
      "step: 411, loss: 0.002197863534092903, accuracy: (0.98783457, 0.98786408)\n",
      "step: 412, loss: 0.002189834602177143, accuracy: (0.98786408, 0.98789346)\n",
      "step: 413, loss: 0.0021818382665514946, accuracy: (0.98789346, 0.98792273)\n",
      "step: 414, loss: 0.0021737259812653065, accuracy: (0.98792273, 0.98795182)\n",
      "step: 415, loss: 0.0021661417558789253, accuracy: (0.98795182, 0.98798078)\n",
      "step: 416, loss: 0.0021577125880867243, accuracy: (0.98798078, 0.98800957)\n",
      "step: 417, loss: 0.0021501625888049603, accuracy: (0.98800957, 0.9880383)\n",
      "step: 418, loss: 0.0021422572899609804, accuracy: (0.9880383, 0.98806685)\n",
      "step: 419, loss: 0.0021349440794438124, accuracy: (0.98806685, 0.98809522)\n",
      "step: 420, loss: 0.002127278596162796, accuracy: (0.98809522, 0.98812354)\n",
      "step: 421, loss: 0.00211961567401886, accuracy: (0.98812354, 0.98815167)\n",
      "step: 422, loss: 0.0021121609024703503, accuracy: (0.98815167, 0.98817968)\n",
      "step: 423, loss: 0.002104405779391527, accuracy: (0.98817968, 0.98820752)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 424, loss: 0.0020972879137843847, accuracy: (0.98820752, 0.98823529)\n",
      "step: 425, loss: 0.0020892638713121414, accuracy: (0.98823529, 0.98826289)\n",
      "step: 426, loss: 0.0020822319202125072, accuracy: (0.98826289, 0.98829037)\n",
      "step: 427, loss: 0.002074667951092124, accuracy: (0.98829037, 0.98831773)\n",
      "step: 428, loss: 0.0020679007284343243, accuracy: (0.98831773, 0.98834497)\n",
      "step: 429, loss: 0.0020604869350790977, accuracy: (0.98834497, 0.98837209)\n",
      "step: 430, loss: 0.0020533925853669643, accuracy: (0.98837209, 0.98839909)\n",
      "step: 431, loss: 0.0020461922977119684, accuracy: (0.98839909, 0.98842591)\n",
      "step: 432, loss: 0.0020390059798955917, accuracy: (0.98842591, 0.98845267)\n",
      "step: 433, loss: 0.002032165415585041, accuracy: (0.98845267, 0.98847926)\n",
      "step: 434, loss: 0.002024666639044881, accuracy: (0.98847926, 0.98850572)\n",
      "step: 435, loss: 0.0020184244494885206, accuracy: (0.98850572, 0.98853213)\n",
      "step: 436, loss: 0.0020107529126107693, accuracy: (0.98853213, 0.98855835)\n",
      "step: 437, loss: 0.0020041365642100573, accuracy: (0.98855835, 0.98858446)\n",
      "step: 438, loss: 0.0019971984438598156, accuracy: (0.98858446, 0.98861051)\n",
      "step: 439, loss: 0.0019906312227249146, accuracy: (0.98861051, 0.98863637)\n",
      "step: 440, loss: 0.0019838926382362843, accuracy: (0.98863637, 0.98866212)\n",
      "step: 441, loss: 0.0019770185463130474, accuracy: (0.98866212, 0.98868775)\n",
      "step: 442, loss: 0.001970558427274227, accuracy: (0.98868775, 0.98871332)\n",
      "step: 443, loss: 0.0019636100623756647, accuracy: (0.98871332, 0.98873872)\n",
      "step: 444, loss: 0.0019573699682950974, accuracy: (0.98873872, 0.98876405)\n",
      "step: 445, loss: 0.001950290985405445, accuracy: (0.98876405, 0.98878926)\n",
      "step: 446, loss: 0.0019439387833699584, accuracy: (0.98878926, 0.98881429)\n",
      "step: 447, loss: 0.0019374529365450144, accuracy: (0.98881429, 0.98883927)\n",
      "step: 448, loss: 0.0019313456723466516, accuracy: (0.98883927, 0.98886412)\n",
      "step: 449, loss: 0.001924644922837615, accuracy: (0.98886412, 0.98888886)\n",
      "step: 450, loss: 0.001918276073411107, accuracy: (0.98888886, 0.98891354)\n",
      "step: 451, loss: 0.0019122259691357613, accuracy: (0.98891354, 0.98893803)\n",
      "step: 452, loss: 0.001905542565509677, accuracy: (0.98893803, 0.98896247)\n",
      "step: 453, loss: 0.0018996933940798044, accuracy: (0.98896247, 0.98898679)\n",
      "step: 454, loss: 0.0018930474761873484, accuracy: (0.98898679, 0.98901099)\n",
      "step: 455, loss: 0.0018868822371587157, accuracy: (0.98901099, 0.98903507)\n",
      "step: 456, loss: 0.0018807284068316221, accuracy: (0.98903507, 0.98905909)\n",
      "step: 457, loss: 0.0018746040295809507, accuracy: (0.98905909, 0.98908299)\n",
      "step: 458, loss: 0.0018687002593651414, accuracy: (0.98908299, 0.98910677)\n",
      "step: 459, loss: 0.0018627855461090803, accuracy: (0.98910677, 0.98913044)\n",
      "step: 460, loss: 0.0018565686186775565, accuracy: (0.98913044, 0.98915404)\n",
      "step: 461, loss: 0.001850796863436699, accuracy: (0.98915404, 0.98917747)\n",
      "step: 462, loss: 0.0018445125315338373, accuracy: (0.98917747, 0.98920089)\n",
      "step: 463, loss: 0.0018390066688880324, accuracy: (0.98920089, 0.98922414)\n",
      "step: 464, loss: 0.001832715468481183, accuracy: (0.98922414, 0.98924732)\n",
      "step: 465, loss: 0.001826928579248488, accuracy: (0.98924732, 0.98927039)\n",
      "step: 466, loss: 0.0018210404086858034, accuracy: (0.98927039, 0.98929334)\n",
      "step: 467, loss: 0.001815589377656579, accuracy: (0.98929334, 0.98931623)\n",
      "step: 468, loss: 0.0018096761777997017, accuracy: (0.98931623, 0.98933899)\n",
      "step: 469, loss: 0.0018040910363197327, accuracy: (0.98933899, 0.9893617)\n",
      "step: 470, loss: 0.0017983491998165846, accuracy: (0.9893617, 0.98938429)\n",
      "step: 471, loss: 0.0017924734856933355, accuracy: (0.98938429, 0.98940676)\n",
      "step: 472, loss: 0.0017873209435492754, accuracy: (0.98940676, 0.98942918)\n",
      "step: 473, loss: 0.0017811987781897187, accuracy: (0.98942918, 0.98945147)\n",
      "step: 474, loss: 0.0017761776689440012, accuracy: (0.98945147, 0.9894737)\n",
      "step: 475, loss: 0.001770111732184887, accuracy: (0.9894737, 0.98949581)\n",
      "step: 476, loss: 0.0017651089001446962, accuracy: (0.98949581, 0.98951781)\n",
      "step: 477, loss: 0.001759308623149991, accuracy: (0.98951781, 0.98953974)\n",
      "step: 478, loss: 0.0017539693508297205, accuracy: (0.98953974, 0.98956156)\n",
      "step: 479, loss: 0.0017485988792032003, accuracy: (0.98956156, 0.98958331)\n",
      "step: 480, loss: 0.001743067754432559, accuracy: (0.98958331, 0.98960501)\n",
      "step: 481, loss: 0.0017379671335220337, accuracy: (0.98960501, 0.98962653)\n",
      "step: 482, loss: 0.0017322020139545202, accuracy: (0.98962653, 0.98964804)\n",
      "step: 483, loss: 0.0017271137330681086, accuracy: (0.98964804, 0.98966944)\n",
      "step: 484, loss: 0.0017217745771631598, accuracy: (0.98966944, 0.98969072)\n",
      "step: 485, loss: 0.0017168771009892225, accuracy: (0.98969072, 0.98971194)\n",
      "step: 486, loss: 0.001711419434286654, accuracy: (0.98971194, 0.98973304)\n",
      "step: 487, loss: 0.0017062574625015259, accuracy: (0.98973304, 0.98975408)\n",
      "step: 488, loss: 0.0017012711614370346, accuracy: (0.98975408, 0.98977506)\n",
      "step: 489, loss: 0.0016959324711933732, accuracy: (0.98977506, 0.98979592)\n",
      "step: 490, loss: 0.001691104145720601, accuracy: (0.98979592, 0.98981673)\n",
      "step: 491, loss: 0.0016856857109814882, accuracy: (0.98981673, 0.98983741)\n",
      "step: 492, loss: 0.0016810346860438585, accuracy: (0.98983741, 0.98985803)\n",
      "step: 493, loss: 0.0016755957622081041, accuracy: (0.98985803, 0.98987854)\n",
      "step: 494, loss: 0.0016709782648831606, accuracy: (0.98987854, 0.98989898)\n",
      "step: 495, loss: 0.0016658119857311249, accuracy: (0.98989898, 0.98991936)\n",
      "step: 496, loss: 0.0016609651502221823, accuracy: (0.98991936, 0.98993963)\n",
      "step: 497, loss: 0.0016560007352381945, accuracy: (0.98993963, 0.98995984)\n",
      "step: 498, loss: 0.0016509514534845948, accuracy: (0.98995984, 0.98997998)\n",
      "step: 499, loss: 0.0016464730724692345, accuracy: (0.98997998, 0.99000001)\n",
      "step: 500, loss: 0.0016412080731242895, accuracy: (0.99000001, 0.99001998)\n",
      "step: 501, loss: 0.001636472879908979, accuracy: (0.99001998, 0.99003983)\n",
      "step: 502, loss: 0.00163163710385561, accuracy: (0.99003983, 0.99005961)\n",
      "step: 503, loss: 0.001627159072086215, accuracy: (0.99005961, 0.99007934)\n",
      "step: 504, loss: 0.0016223473940044641, accuracy: (0.99007934, 0.99009901)\n",
      "step: 505, loss: 0.0016176039353013039, accuracy: (0.99009901, 0.99011856)\n",
      "step: 506, loss: 0.0016129823634400964, accuracy: (0.99011856, 0.99013805)\n",
      "step: 507, loss: 0.00160819455049932, accuracy: (0.99013805, 0.99015749)\n",
      "step: 508, loss: 0.001603718614205718, accuracy: (0.99015749, 0.9901768)\n",
      "step: 509, loss: 0.0015988729428499937, accuracy: (0.9901768, 0.99019605)\n",
      "step: 510, loss: 0.0015942465979605913, accuracy: (0.99019605, 0.99021524)\n",
      "step: 511, loss: 0.0015896728727966547, accuracy: (0.99021524, 0.99023438)\n",
      "step: 512, loss: 0.0015851454809308052, accuracy: (0.99023438, 0.99025339)\n",
      "step: 513, loss: 0.0015808036550879478, accuracy: (0.99025339, 0.9902724)\n",
      "step: 514, loss: 0.0015762790571898222, accuracy: (0.9902724, 0.99029124)\n",
      "step: 515, loss: 0.0015717748319730163, accuracy: (0.99029124, 0.99031007)\n",
      "step: 516, loss: 0.0015673950547352433, accuracy: (0.99031007, 0.99032885)\n",
      "step: 517, loss: 0.0015628200490027666, accuracy: (0.99032885, 0.9903475)\n",
      "step: 518, loss: 0.0015585932414978743, accuracy: (0.9903475, 0.9903661)\n",
      "step: 519, loss: 0.001553859794512391, accuracy: (0.9903661, 0.99038464)\n",
      "step: 520, loss: 0.0015496397390961647, accuracy: (0.99038464, 0.99040306)\n",
      "step: 521, loss: 0.0015452788211405277, accuracy: (0.99040306, 0.99042147)\n",
      "step: 522, loss: 0.001541173318400979, accuracy: (0.99042147, 0.99043977)\n",
      "step: 523, loss: 0.0015366855077445507, accuracy: (0.99043977, 0.99045801)\n",
      "step: 524, loss: 0.0015323974657803774, accuracy: (0.99045801, 0.99047619)\n",
      "step: 525, loss: 0.0015283379470929503, accuracy: (0.99047619, 0.99049431)\n",
      "step: 526, loss: 0.001523821847513318, accuracy: (0.99049431, 0.99051231)\n",
      "step: 527, loss: 0.0015199061017483473, accuracy: (0.99051231, 0.99053031)\n",
      "step: 528, loss: 0.001515338197350502, accuracy: (0.99053031, 0.99054819)\n",
      "step: 529, loss: 0.0015113165136426687, accuracy: (0.99054819, 0.99056602)\n",
      "step: 530, loss: 0.0015070760855451226, accuracy: (0.99056602, 0.99058378)\n",
      "step: 531, loss: 0.0015031183138489723, accuracy: (0.99058378, 0.99060148)\n",
      "step: 532, loss: 0.0014989660121500492, accuracy: (0.99060148, 0.99061912)\n",
      "step: 533, loss: 0.001494864234700799, accuracy: (0.99061912, 0.99063671)\n",
      "step: 534, loss: 0.0014908274170011282, accuracy: (0.99063671, 0.99065423)\n",
      "step: 535, loss: 0.0014866641722619534, accuracy: (0.99065423, 0.99067163)\n",
      "step: 536, loss: 0.001482784515246749, accuracy: (0.99067163, 0.99068904)\n",
      "step: 537, loss: 0.0014784806407988071, accuracy: (0.99068904, 0.99070632)\n",
      "step: 538, loss: 0.0014745850348845124, accuracy: (0.99070632, 0.99072355)\n",
      "step: 539, loss: 0.0014705831417813897, accuracy: (0.99072355, 0.99074072)\n",
      "step: 540, loss: 0.0014668407384306192, accuracy: (0.99074072, 0.99075788)\n",
      "step: 541, loss: 0.0014627352356910706, accuracy: (0.99075788, 0.99077493)\n",
      "step: 542, loss: 0.0014588164631277323, accuracy: (0.99077493, 0.99079192)\n",
      "step: 543, loss: 0.0014550630003213882, accuracy: (0.99079192, 0.99080884)\n",
      "step: 544, loss: 0.0014509368920698762, accuracy: (0.99080884, 0.99082571)\n",
      "step: 545, loss: 0.0014473369810730219, accuracy: (0.99082571, 0.99084246)\n",
      "step: 546, loss: 0.0014432099414989352, accuracy: (0.99084246, 0.99085921)\n",
      "step: 547, loss: 0.001439443090930581, accuracy: (0.99085921, 0.9908759)\n",
      "step: 548, loss: 0.0014355750754475594, accuracy: (0.9908759, 0.99089253)\n",
      "step: 549, loss: 0.0014319645706564188, accuracy: (0.99089253, 0.9909091)\n",
      "step: 550, loss: 0.0014281432377174497, accuracy: (0.9909091, 0.99092561)\n",
      "step: 551, loss: 0.0014243770856410265, accuracy: (0.99092561, 0.990942)\n",
      "step: 552, loss: 0.0014206658815965056, accuracy: (0.990942, 0.99095839)\n",
      "step: 553, loss: 0.0014167618937790394, accuracy: (0.99095839, 0.99097472)\n",
      "step: 554, loss: 0.0014130612835288048, accuracy: (0.99097472, 0.990991)\n",
      "step: 555, loss: 0.0014093660283833742, accuracy: (0.990991, 0.99100721)\n",
      "step: 556, loss: 0.0014057217631489038, accuracy: (0.99100721, 0.99102336)\n",
      "step: 557, loss: 0.0014022956602275372, accuracy: (0.99102336, 0.99103945)\n",
      "step: 558, loss: 0.0013984537217766047, accuracy: (0.99103945, 0.99105543)\n",
      "step: 559, loss: 0.0013948946725577116, accuracy: (0.99105543, 0.9910714)\n",
      "step: 560, loss: 0.0013913882430642843, accuracy: (0.9910714, 0.99108732)\n",
      "step: 561, loss: 0.0013876404846087098, accuracy: (0.99108732, 0.99110323)\n",
      "step: 562, loss: 0.0013842469779774547, accuracy: (0.99110323, 0.99111903)\n",
      "step: 563, loss: 0.0013804435729980469, accuracy: (0.99111903, 0.99113476)\n",
      "step: 564, loss: 0.001377190463244915, accuracy: (0.99113476, 0.99115044)\n",
      "step: 565, loss: 0.001373350154608488, accuracy: (0.99115044, 0.99116606)\n",
      "step: 566, loss: 0.0013699561823159456, accuracy: (0.99116606, 0.99118167)\n",
      "step: 567, loss: 0.001366426469758153, accuracy: (0.99118167, 0.99119717)\n",
      "step: 568, loss: 0.0013630683533847332, accuracy: (0.99119717, 0.99121267)\n",
      "step: 569, loss: 0.001359596149995923, accuracy: (0.99121267, 0.99122804)\n",
      "step: 570, loss: 0.0013560818042606115, accuracy: (0.99122804, 0.99124342)\n",
      "step: 571, loss: 0.0013527569826692343, accuracy: (0.99124342, 0.99125874)\n",
      "step: 572, loss: 0.001349105266854167, accuracy: (0.99125874, 0.991274)\n",
      "step: 573, loss: 0.0013458238681778312, accuracy: (0.991274, 0.9912892)\n",
      "step: 574, loss: 0.0013423976488411427, accuracy: (0.9912892, 0.99130434)\n",
      "step: 575, loss: 0.0013392230030149221, accuracy: (0.99130434, 0.99131942)\n",
      "step: 576, loss: 0.001335719134658575, accuracy: (0.99131942, 0.9913345)\n",
      "step: 577, loss: 0.001332395477220416, accuracy: (0.9913345, 0.99134946)\n",
      "step: 578, loss: 0.0013291921932250261, accuracy: (0.99134946, 0.99136442)\n",
      "step: 579, loss: 0.0013256840175017715, accuracy: (0.99136442, 0.99137932)\n",
      "step: 580, loss: 0.0013226168230175972, accuracy: (0.99137932, 0.99139416)\n",
      "step: 581, loss: 0.0013190830359235406, accuracy: (0.99139416, 0.99140894)\n",
      "step: 582, loss: 0.001316094771027565, accuracy: (0.99140894, 0.99142367)\n",
      "step: 583, loss: 0.0013125884579494596, accuracy: (0.99142367, 0.99143833)\n",
      "step: 584, loss: 0.0013093688758090138, accuracy: (0.99143833, 0.99145299)\n",
      "step: 585, loss: 0.0013061817735433578, accuracy: (0.99145299, 0.9914676)\n",
      "step: 586, loss: 0.00130304170306772, accuracy: (0.9914676, 0.99148214)\n",
      "step: 587, loss: 0.001299859257414937, accuracy: (0.99148214, 0.99149662)\n",
      "step: 588, loss: 0.0012965898495167494, accuracy: (0.99149662, 0.99151105)\n",
      "step: 589, loss: 0.0012935614213347435, accuracy: (0.99151105, 0.99152541)\n",
      "step: 590, loss: 0.0012902127346023917, accuracy: (0.99152541, 0.99153978)\n",
      "step: 591, loss: 0.0012870790669694543, accuracy: (0.99153978, 0.99155408)\n",
      "step: 592, loss: 0.0012839778792113066, accuracy: (0.99155408, 0.99156827)\n",
      "step: 593, loss: 0.0012810141779482365, accuracy: (0.99156827, 0.99158251)\n",
      "step: 594, loss: 0.0012778212549164891, accuracy: (0.99158251, 0.99159664)\n",
      "step: 595, loss: 0.0012747005093842745, accuracy: (0.99159664, 0.99161077)\n",
      "step: 596, loss: 0.0012717714998871088, accuracy: (0.99161077, 0.99162477)\n",
      "step: 597, loss: 0.0012685145484283566, accuracy: (0.99162477, 0.99163878)\n",
      "step: 598, loss: 0.0012654908932745457, accuracy: (0.99163878, 0.99165273)\n",
      "step: 599, loss: 0.0012624484952539206, accuracy: (0.99165273, 0.99166667)\n",
      "step: 600, loss: 0.0012594392756000161, accuracy: (0.99166667, 0.9916805)\n",
      "step: 601, loss: 0.0012566016521304846, accuracy: (0.9916805, 0.99169433)\n",
      "step: 602, loss: 0.0012534676352515817, accuracy: (0.99169433, 0.9917081)\n",
      "step: 603, loss: 0.0012505820486694574, accuracy: (0.9917081, 0.99172187)\n",
      "step: 604, loss: 0.0012475554831326008, accuracy: (0.99172187, 0.99173552)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 605, loss: 0.0012445047032088041, accuracy: (0.99173552, 0.99174917)\n",
      "step: 606, loss: 0.0012417675461620092, accuracy: (0.99174917, 0.99176276)\n",
      "step: 607, loss: 0.0012385996524244547, accuracy: (0.99176276, 0.99177629)\n",
      "step: 608, loss: 0.0012357691302895546, accuracy: (0.99177629, 0.99178982)\n",
      "step: 609, loss: 0.0012327770236879587, accuracy: (0.99178982, 0.99180329)\n",
      "step: 610, loss: 0.0012300326488912106, accuracy: (0.99180329, 0.9918167)\n",
      "step: 611, loss: 0.0012271327432245016, accuracy: (0.9918167, 0.99183005)\n",
      "step: 612, loss: 0.0012242377270013094, accuracy: (0.99183005, 0.9918434)\n",
      "step: 613, loss: 0.0012214151211082935, accuracy: (0.9918434, 0.99185669)\n",
      "step: 614, loss: 0.001218460383825004, accuracy: (0.99185669, 0.99186993)\n",
      "step: 615, loss: 0.0012157608289271593, accuracy: (0.99186993, 0.9918831)\n",
      "step: 616, loss: 0.0012127631343901157, accuracy: (0.9918831, 0.99189627)\n",
      "step: 617, loss: 0.0012101707980036736, accuracy: (0.99189627, 0.99190938)\n",
      "step: 618, loss: 0.0012071391101926565, accuracy: (0.99190938, 0.99192244)\n",
      "step: 619, loss: 0.0012045081239193678, accuracy: (0.99192244, 0.99193549)\n",
      "step: 620, loss: 0.0012016682885587215, accuracy: (0.99193549, 0.99194849)\n",
      "step: 621, loss: 0.0011989298509433866, accuracy: (0.99194849, 0.99196142)\n",
      "step: 622, loss: 0.0011961658019572496, accuracy: (0.99196142, 0.99197429)\n",
      "step: 623, loss: 0.0011933138594031334, accuracy: (0.99197429, 0.99198717)\n",
      "step: 624, loss: 0.0011905921855941415, accuracy: (0.99198717, 0.99199998)\n",
      "step: 625, loss: 0.0011878667864948511, accuracy: (0.99199998, 0.9920128)\n",
      "step: 626, loss: 0.0011851562885567546, accuracy: (0.9920128, 0.99202549)\n",
      "step: 627, loss: 0.001182446489110589, accuracy: (0.99202549, 0.99203819)\n",
      "step: 628, loss: 0.0011798841878771782, accuracy: (0.99203819, 0.99205089)\n",
      "step: 629, loss: 0.0011771826539188623, accuracy: (0.99205089, 0.99206346)\n",
      "step: 630, loss: 0.001174498233012855, accuracy: (0.99206346, 0.9920761)\n",
      "step: 631, loss: 0.0011718756286427379, accuracy: (0.9920761, 0.99208862)\n",
      "step: 632, loss: 0.001169137773104012, accuracy: (0.99208862, 0.99210113)\n",
      "step: 633, loss: 0.0011664819903671741, accuracy: (0.99210113, 0.99211359)\n",
      "step: 634, loss: 0.0011638638097792864, accuracy: (0.99211359, 0.99212599)\n",
      "step: 635, loss: 0.0011612239759415388, accuracy: (0.99212599, 0.99213839)\n",
      "step: 636, loss: 0.0011587541084736586, accuracy: (0.99213839, 0.99215072)\n",
      "step: 637, loss: 0.001156143262051046, accuracy: (0.99215072, 0.992163)\n",
      "step: 638, loss: 0.0011535445228219032, accuracy: (0.992163, 0.99217528)\n",
      "step: 639, loss: 0.0011510004987940192, accuracy: (0.99217528, 0.9921875)\n",
      "step: 640, loss: 0.0011483679991215467, accuracy: (0.9921875, 0.99219966)\n",
      "step: 641, loss: 0.001145910588093102, accuracy: (0.99219966, 0.99221182)\n",
      "step: 642, loss: 0.0011432368773967028, accuracy: (0.99221182, 0.99222398)\n",
      "step: 643, loss: 0.0011408792342990637, accuracy: (0.99222398, 0.99223602)\n",
      "step: 644, loss: 0.0011381524382159114, accuracy: (0.99223602, 0.99224806)\n",
      "step: 645, loss: 0.0011358008487150073, accuracy: (0.99224806, 0.99226004)\n",
      "step: 646, loss: 0.0011332223657518625, accuracy: (0.99226004, 0.99227202)\n",
      "step: 647, loss: 0.0011307697277516127, accuracy: (0.99227202, 0.99228394)\n",
      "step: 648, loss: 0.0011282714549452066, accuracy: (0.99228394, 0.99229586)\n",
      "step: 649, loss: 0.0011256858706474304, accuracy: (0.99229586, 0.99230766)\n",
      "step: 650, loss: 0.0011234376579523087, accuracy: (0.99230766, 0.99231952)\n",
      "step: 651, loss: 0.001120752189308405, accuracy: (0.99231952, 0.99233127)\n",
      "step: 652, loss: 0.0011183790629729629, accuracy: (0.99233127, 0.99234301)\n",
      "step: 653, loss: 0.001115906983613968, accuracy: (0.99234301, 0.99235475)\n",
      "step: 654, loss: 0.001113568781875074, accuracy: (0.99235475, 0.99236643)\n",
      "step: 655, loss: 0.0011111784260720015, accuracy: (0.99236643, 0.99237806)\n",
      "step: 656, loss: 0.0011087094899266958, accuracy: (0.99237806, 0.99238968)\n",
      "step: 657, loss: 0.0011063899146392941, accuracy: (0.99238968, 0.99240124)\n",
      "step: 658, loss: 0.0011038670781999826, accuracy: (0.99240124, 0.99241275)\n",
      "step: 659, loss: 0.0011016555363312364, accuracy: (0.99241275, 0.99242425)\n",
      "step: 660, loss: 0.001099128625355661, accuracy: (0.99242425, 0.99243569)\n",
      "step: 661, loss: 0.0010967638809233904, accuracy: (0.99243569, 0.99244714)\n",
      "step: 662, loss: 0.0010944099631160498, accuracy: (0.99244714, 0.99245852)\n",
      "step: 663, loss: 0.0010921547655016184, accuracy: (0.99245852, 0.99246991)\n",
      "step: 664, loss: 0.0010898096952587366, accuracy: (0.99246991, 0.99248123)\n",
      "step: 665, loss: 0.001087456475943327, accuracy: (0.99248123, 0.9924925)\n",
      "step: 666, loss: 0.0010851930128410459, accuracy: (0.9924925, 0.99250376)\n",
      "step: 667, loss: 0.001082768663764, accuracy: (0.99250376, 0.99251497)\n",
      "step: 668, loss: 0.0010804852936416864, accuracy: (0.99251497, 0.99252617)\n",
      "step: 669, loss: 0.001078214030712843, accuracy: (0.99252617, 0.99253732)\n",
      "step: 670, loss: 0.0010758982971310616, accuracy: (0.99253732, 0.99254841)\n",
      "step: 671, loss: 0.0010737464763224125, accuracy: (0.99254841, 0.99255955)\n",
      "step: 672, loss: 0.001071438193321228, accuracy: (0.99255955, 0.99257058)\n",
      "step: 673, loss: 0.0010691931238397956, accuracy: (0.99257058, 0.99258161)\n",
      "step: 674, loss: 0.0010669571347534657, accuracy: (0.99258161, 0.99259257)\n",
      "step: 675, loss: 0.0010645946022123098, accuracy: (0.99259257, 0.99260354)\n",
      "step: 676, loss: 0.0010625813156366348, accuracy: (0.99260354, 0.99261445)\n",
      "step: 677, loss: 0.00106014683842659, accuracy: (0.99261445, 0.99262536)\n",
      "step: 678, loss: 0.001057991525158286, accuracy: (0.99262536, 0.9926362)\n",
      "step: 679, loss: 0.001055761706084013, accuracy: (0.9926362, 0.99264705)\n",
      "step: 680, loss: 0.0010536849731579423, accuracy: (0.99264705, 0.99265784)\n",
      "step: 681, loss: 0.001051425701007247, accuracy: (0.99265784, 0.99266863)\n",
      "step: 682, loss: 0.0010492217261344194, accuracy: (0.99266863, 0.99267936)\n",
      "step: 683, loss: 0.0010471397545188665, accuracy: (0.99267936, 0.99269009)\n",
      "step: 684, loss: 0.0010448312386870384, accuracy: (0.99269009, 0.99270076)\n",
      "step: 685, loss: 0.0010428467066958547, accuracy: (0.99270076, 0.99271137)\n",
      "step: 686, loss: 0.0010405308566987514, accuracy: (0.99271137, 0.99272197)\n",
      "step: 687, loss: 0.0010384127963334322, accuracy: (0.99272197, 0.99273258)\n",
      "step: 688, loss: 0.0010362735483795404, accuracy: (0.99273258, 0.99274313)\n",
      "step: 689, loss: 0.0010342036839574575, accuracy: (0.99274313, 0.99275362)\n",
      "step: 690, loss: 0.0010321044828742743, accuracy: (0.99275362, 0.99276412)\n",
      "step: 691, loss: 0.00102994404733181, accuracy: (0.99276412, 0.99277455)\n",
      "step: 692, loss: 0.0010279089910909534, accuracy: (0.99277455, 0.99278498)\n",
      "step: 693, loss: 0.001025682664476335, accuracy: (0.99278498, 0.99279541)\n",
      "step: 694, loss: 0.0010236401576548815, accuracy: (0.99279541, 0.99280578)\n",
      "step: 695, loss: 0.0010215366492047906, accuracy: (0.99280578, 0.99281609)\n",
      "step: 696, loss: 0.0010195017093792558, accuracy: (0.99281609, 0.9928264)\n",
      "step: 697, loss: 0.0010175022762268782, accuracy: (0.9928264, 0.99283665)\n",
      "step: 698, loss: 0.001015407033264637, accuracy: (0.99283665, 0.99284691)\n",
      "step: 699, loss: 0.001013366156257689, accuracy: (0.99284691, 0.99285716)\n",
      "step: 700, loss: 0.0010113436728715897, accuracy: (0.99285716, 0.99286735)\n",
      "step: 701, loss: 0.00100919627584517, accuracy: (0.99286735, 0.99287748)\n",
      "step: 702, loss: 0.0010072160512208939, accuracy: (0.99287748, 0.99288762)\n",
      "step: 703, loss: 0.0010051648132503033, accuracy: (0.99288762, 0.99289775)\n",
      "step: 704, loss: 0.0010033075232058764, accuracy: (0.99289775, 0.99290782)\n",
      "step: 705, loss: 0.0010011664126068354, accuracy: (0.99290782, 0.99291784)\n",
      "step: 706, loss: 0.0009992006234824657, accuracy: (0.99291784, 0.99292785)\n",
      "step: 707, loss: 0.000997270573861897, accuracy: (0.99292785, 0.99293786)\n",
      "step: 708, loss: 0.0009951938409358263, accuracy: (0.99293786, 0.99294782)\n",
      "step: 709, loss: 0.0009933151304721832, accuracy: (0.99294782, 0.99295777)\n",
      "step: 710, loss: 0.0009911981178447604, accuracy: (0.99295777, 0.99296767)\n",
      "step: 711, loss: 0.000989405671134591, accuracy: (0.99296767, 0.9929775)\n",
      "step: 712, loss: 0.0009872813243418932, accuracy: (0.9929775, 0.99298739)\n",
      "step: 713, loss: 0.000985500169917941, accuracy: (0.99298739, 0.99299717)\n",
      "step: 714, loss: 0.0009834310039877892, accuracy: (0.99299717, 0.993007)\n",
      "step: 715, loss: 0.0009815231896936893, accuracy: (0.993007, 0.99301678)\n",
      "step: 716, loss: 0.0009796274825930595, accuracy: (0.99301678, 0.99302649)\n",
      "step: 717, loss: 0.00097764958627522, accuracy: (0.99302649, 0.99303621)\n",
      "step: 718, loss: 0.0009758076630532742, accuracy: (0.99303621, 0.99304593)\n",
      "step: 719, loss: 0.0009737889049574733, accuracy: (0.99304593, 0.99305558)\n",
      "step: 720, loss: 0.0009718846413306892, accuracy: (0.99305558, 0.99306518)\n",
      "step: 721, loss: 0.0009699968504719436, accuracy: (0.99306518, 0.99307477)\n",
      "step: 722, loss: 0.0009681869996711612, accuracy: (0.99307477, 0.99308437)\n",
      "step: 723, loss: 0.000966295599937439, accuracy: (0.99308437, 0.99309391)\n",
      "step: 724, loss: 0.0009643860394135118, accuracy: (0.99309391, 0.99310344)\n",
      "step: 725, loss: 0.0009625728125683963, accuracy: (0.99310344, 0.99311292)\n",
      "step: 726, loss: 0.0009606407838873565, accuracy: (0.99311292, 0.9931224)\n",
      "step: 727, loss: 0.000958738848567009, accuracy: (0.9931224, 0.99313188)\n",
      "step: 728, loss: 0.0009569089743308723, accuracy: (0.99313188, 0.99314129)\n",
      "step: 729, loss: 0.0009550647810101509, accuracy: (0.99314129, 0.99315071)\n",
      "step: 730, loss: 0.0009533468401059508, accuracy: (0.99315071, 0.99316007)\n",
      "step: 731, loss: 0.0009514203993603587, accuracy: (0.99316007, 0.99316943)\n",
      "step: 732, loss: 0.000949595938436687, accuracy: (0.99316943, 0.99317873)\n",
      "step: 733, loss: 0.0009478546562604606, accuracy: (0.99317873, 0.99318802)\n",
      "step: 734, loss: 0.0009459472494199872, accuracy: (0.99318802, 0.99319726)\n",
      "step: 735, loss: 0.0009442435693927109, accuracy: (0.99319726, 0.9932065)\n",
      "step: 736, loss: 0.0009423060109838843, accuracy: (0.9932065, 0.99321574)\n",
      "step: 737, loss: 0.0009405611199326813, accuracy: (0.99321574, 0.99322492)\n",
      "step: 738, loss: 0.0009387480095028877, accuracy: (0.99322492, 0.9932341)\n",
      "step: 739, loss: 0.0009370461339130998, accuracy: (0.9932341, 0.99324322)\n",
      "step: 740, loss: 0.0009352479828521609, accuracy: (0.99324322, 0.99325234)\n",
      "step: 741, loss: 0.0009334564092569053, accuracy: (0.99325234, 0.99326146)\n",
      "step: 742, loss: 0.0009317434160038829, accuracy: (0.99326146, 0.99327052)\n",
      "step: 743, loss: 0.000929909001570195, accuracy: (0.99327052, 0.99327958)\n",
      "step: 744, loss: 0.0009282486280426383, accuracy: (0.99327958, 0.99328858)\n",
      "step: 745, loss: 0.0009263699175789952, accuracy: (0.99328858, 0.99329758)\n",
      "step: 746, loss: 0.0009246563422493637, accuracy: (0.99329758, 0.99330658)\n",
      "step: 747, loss: 0.0009229514980688691, accuracy: (0.99330658, 0.99331552)\n",
      "step: 748, loss: 0.0009212666191160679, accuracy: (0.99331552, 0.99332446)\n",
      "step: 749, loss: 0.000919514219276607, accuracy: (0.99332446, 0.99333334)\n",
      "step: 750, loss: 0.0009177423780784011, accuracy: (0.99333334, 0.99334222)\n",
      "step: 751, loss: 0.0009161654161289334, accuracy: (0.99334222, 0.99335104)\n",
      "step: 752, loss: 0.0009143302449956536, accuracy: (0.99335104, 0.99335992)\n",
      "step: 753, loss: 0.0009126337245106697, accuracy: (0.99335992, 0.99336869)\n",
      "step: 754, loss: 0.000910936389118433, accuracy: (0.99336869, 0.99337751)\n",
      "step: 755, loss: 0.0009092512773349881, accuracy: (0.99337751, 0.99338627)\n",
      "step: 756, loss: 0.0009076534770429134, accuracy: (0.99338627, 0.99339497)\n",
      "step: 757, loss: 0.0009059264557436109, accuracy: (0.99339497, 0.99340367)\n",
      "step: 758, loss: 0.0009042185847647488, accuracy: (0.99340367, 0.99341238)\n",
      "step: 759, loss: 0.000902669969946146, accuracy: (0.99341238, 0.99342108)\n",
      "step: 760, loss: 0.0009008655324578285, accuracy: (0.99342108, 0.99342972)\n",
      "step: 761, loss: 0.0008993637748062611, accuracy: (0.99342972, 0.9934383)\n",
      "step: 762, loss: 0.0008975538657978177, accuracy: (0.9934383, 0.99344695)\n",
      "step: 763, loss: 0.0008960427949205041, accuracy: (0.99344695, 0.99345547)\n",
      "step: 764, loss: 0.0008943165885284543, accuracy: (0.99345547, 0.99346405)\n",
      "step: 765, loss: 0.0008927054586820304, accuracy: (0.99346405, 0.99347258)\n",
      "step: 766, loss: 0.0008911128388717771, accuracy: (0.99347258, 0.9934811)\n",
      "step: 767, loss: 0.0008894233033061028, accuracy: (0.9934811, 0.99348956)\n",
      "step: 768, loss: 0.0008878976223058999, accuracy: (0.99348956, 0.99349803)\n",
      "step: 769, loss: 0.0008861765381880105, accuracy: (0.99349803, 0.99350649)\n",
      "step: 770, loss: 0.0008845814736559987, accuracy: (0.99350649, 0.9935149)\n",
      "step: 771, loss: 0.0008829566650092602, accuracy: (0.9935149, 0.9935233)\n",
      "step: 772, loss: 0.0008814447792246938, accuracy: (0.9935233, 0.9935317)\n",
      "step: 773, loss: 0.000879832252394408, accuracy: (0.9935317, 0.99354005)\n",
      "step: 774, loss: 0.0008782363729551435, accuracy: (0.99354005, 0.99354839)\n",
      "step: 775, loss: 0.0008766755345277488, accuracy: (0.99354839, 0.99355668)\n",
      "step: 776, loss: 0.0008750405395403504, accuracy: (0.99355668, 0.99356502)\n",
      "step: 777, loss: 0.0008735540322959423, accuracy: (0.99356502, 0.99357325)\n",
      "step: 778, loss: 0.0008718888275325298, accuracy: (0.99357325, 0.99358153)\n",
      "step: 779, loss: 0.0008703232742846012, accuracy: (0.99358153, 0.99358976)\n",
      "step: 780, loss: 0.0008688782108947635, accuracy: (0.99358976, 0.99359792)\n",
      "step: 781, loss: 0.0008672476978972554, accuracy: (0.99359792, 0.99360615)\n",
      "step: 782, loss: 0.0008657202124595642, accuracy: (0.99360615, 0.99361432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 783, loss: 0.0008642099564895034, accuracy: (0.99361432, 0.99362242)\n",
      "step: 784, loss: 0.000862611923366785, accuracy: (0.99362242, 0.99363059)\n",
      "step: 785, loss: 0.0008611609227955341, accuracy: (0.99363059, 0.99363869)\n",
      "step: 786, loss: 0.0008595295948907733, accuracy: (0.99363869, 0.99364674)\n",
      "step: 787, loss: 0.000858127255924046, accuracy: (0.99364674, 0.99365485)\n",
      "step: 788, loss: 0.0008564909803681076, accuracy: (0.99365485, 0.99366289)\n",
      "step: 789, loss: 0.0008550502243451774, accuracy: (0.99366289, 0.99367088)\n",
      "step: 790, loss: 0.0008535337401553988, accuracy: (0.99367088, 0.99367887)\n",
      "step: 791, loss: 0.0008520171977579594, accuracy: (0.99367887, 0.99368685)\n",
      "step: 792, loss: 0.0008505379082635045, accuracy: (0.99368685, 0.99369484)\n",
      "step: 793, loss: 0.000848989759106189, accuracy: (0.99369484, 0.99370277)\n",
      "step: 794, loss: 0.0008475842769257724, accuracy: (0.99370277, 0.9937107)\n",
      "step: 795, loss: 0.000845985661726445, accuracy: (0.9937107, 0.99371856)\n",
      "step: 796, loss: 0.0008446216816082597, accuracy: (0.99371856, 0.99372649)\n",
      "step: 797, loss: 0.0008430721354670823, accuracy: (0.99372649, 0.99373436)\n",
      "step: 798, loss: 0.0008416416822001338, accuracy: (0.99373436, 0.99374217)\n",
      "step: 799, loss: 0.0008401592494919896, accuracy: (0.99374217, 0.99374998)\n",
      "step: 800, loss: 0.0008386519039049745, accuracy: (0.99374998, 0.99375778)\n",
      "step: 801, loss: 0.0008372992160730064, accuracy: (0.99375778, 0.99376559)\n",
      "step: 802, loss: 0.0008357295882888138, accuracy: (0.99376559, 0.99377334)\n",
      "step: 803, loss: 0.0008342916262336075, accuracy: (0.99377334, 0.99378109)\n",
      "step: 804, loss: 0.0008328518597409129, accuracy: (0.99378109, 0.99378884)\n",
      "step: 805, loss: 0.00083150714635849, accuracy: (0.99378884, 0.99379653)\n",
      "step: 806, loss: 0.0008299974724650383, accuracy: (0.99379653, 0.99380422)\n",
      "step: 807, loss: 0.0008285751682706177, accuracy: (0.99380422, 0.99381191)\n",
      "step: 808, loss: 0.0008272028644569218, accuracy: (0.99381191, 0.99381953)\n",
      "step: 809, loss: 0.0008257186273112893, accuracy: (0.99381953, 0.99382716)\n",
      "step: 810, loss: 0.0008242709445767105, accuracy: (0.99382716, 0.99383479)\n",
      "step: 811, loss: 0.0008228890364989638, accuracy: (0.99383479, 0.99384236)\n",
      "step: 812, loss: 0.0008214486879296601, accuracy: (0.99384236, 0.99384993)\n",
      "step: 813, loss: 0.0008201604941859841, accuracy: (0.99384993, 0.9938575)\n",
      "step: 814, loss: 0.0008186761988326907, accuracy: (0.9938575, 0.99386501)\n",
      "step: 815, loss: 0.000817294989246875, accuracy: (0.99386501, 0.99387252)\n",
      "step: 816, loss: 0.0008159562712535262, accuracy: (0.99387252, 0.99388003)\n",
      "step: 817, loss: 0.0008144959574565291, accuracy: (0.99388003, 0.99388754)\n",
      "step: 818, loss: 0.000813193735666573, accuracy: (0.99388754, 0.99389499)\n",
      "step: 819, loss: 0.0008117025718092918, accuracy: (0.99389499, 0.99390244)\n",
      "step: 820, loss: 0.0008104586158879101, accuracy: (0.99390244, 0.9939099)\n",
      "step: 821, loss: 0.0008089904440566897, accuracy: (0.9939099, 0.99391729)\n",
      "step: 822, loss: 0.0008076213416643441, accuracy: (0.99391729, 0.99392468)\n",
      "step: 823, loss: 0.0008063229615800083, accuracy: (0.99392468, 0.99393207)\n",
      "step: 824, loss: 0.0008049366297200322, accuracy: (0.99393207, 0.9939394)\n",
      "step: 825, loss: 0.0008035547798499465, accuracy: (0.9939394, 0.99394673)\n",
      "step: 826, loss: 0.0008022913825698197, accuracy: (0.99394673, 0.99395406)\n",
      "step: 827, loss: 0.000800846319179982, accuracy: (0.99395406, 0.99396133)\n",
      "step: 828, loss: 0.0007995043997652829, accuracy: (0.99396133, 0.99396867)\n",
      "step: 829, loss: 0.000798175111413002, accuracy: (0.99396867, 0.99397588)\n",
      "step: 830, loss: 0.0007969285361468792, accuracy: (0.99397588, 0.99398315)\n",
      "step: 831, loss: 0.000795527535956353, accuracy: (0.99398315, 0.99399036)\n",
      "step: 832, loss: 0.0007942043012008071, accuracy: (0.99399036, 0.99399757)\n",
      "step: 833, loss: 0.0007929374696686864, accuracy: (0.99399757, 0.99400479)\n",
      "step: 834, loss: 0.0007915470050647855, accuracy: (0.99400479, 0.994012)\n",
      "step: 835, loss: 0.0007903164951130748, accuracy: (0.994012, 0.99401915)\n",
      "step: 836, loss: 0.0007889012922532856, accuracy: (0.99401915, 0.9940263)\n",
      "step: 837, loss: 0.0007876399904489517, accuracy: (0.9940263, 0.9940334)\n",
      "step: 838, loss: 0.0007863165810704231, accuracy: (0.9940334, 0.99404055)\n",
      "step: 839, loss: 0.0007850672118365765, accuracy: (0.99404055, 0.99404764)\n",
      "step: 840, loss: 0.0007837681914679706, accuracy: (0.99404764, 0.99405468)\n",
      "step: 841, loss: 0.0007824504864402115, accuracy: (0.99405468, 0.99406177)\n",
      "step: 842, loss: 0.0007812114781700075, accuracy: (0.99406177, 0.9940688)\n",
      "step: 843, loss: 0.0007798600709065795, accuracy: (0.9940688, 0.99407583)\n",
      "step: 844, loss: 0.000778661691583693, accuracy: (0.99407583, 0.99408287)\n",
      "step: 845, loss: 0.0007772876415401697, accuracy: (0.99408287, 0.99408984)\n",
      "step: 846, loss: 0.000776098866481334, accuracy: (0.99408984, 0.99409682)\n",
      "step: 847, loss: 0.0007747964700683951, accuracy: (0.99409682, 0.99410379)\n",
      "step: 848, loss: 0.0007735462859272957, accuracy: (0.99410379, 0.9941107)\n",
      "step: 849, loss: 0.0007722830632701516, accuracy: (0.9941107, 0.99411762)\n",
      "step: 850, loss: 0.0007710010977461934, accuracy: (0.99411762, 0.99412453)\n",
      "step: 851, loss: 0.0007697982946410775, accuracy: (0.99412453, 0.99413145)\n",
      "step: 852, loss: 0.000768492347560823, accuracy: (0.99413145, 0.99413836)\n",
      "step: 853, loss: 0.0007672185311093926, accuracy: (0.99413836, 0.99414521)\n",
      "step: 854, loss: 0.0007660578703507781, accuracy: (0.99414521, 0.99415207)\n",
      "step: 855, loss: 0.0007647979655303061, accuracy: (0.99415207, 0.99415886)\n",
      "step: 856, loss: 0.0007635653018951416, accuracy: (0.99415886, 0.99416572)\n",
      "step: 857, loss: 0.0007623403216712177, accuracy: (0.99416572, 0.99417251)\n",
      "step: 858, loss: 0.0007610763423144817, accuracy: (0.99417251, 0.99417925)\n",
      "step: 859, loss: 0.0007599021773785353, accuracy: (0.99417925, 0.99418604)\n",
      "step: 860, loss: 0.0007586129941046238, accuracy: (0.99418604, 0.99419278)\n",
      "step: 861, loss: 0.0007574820192530751, accuracy: (0.99419278, 0.99419951)\n",
      "step: 862, loss: 0.0007561748498119414, accuracy: (0.99419951, 0.99420625)\n",
      "step: 863, loss: 0.0007550281006842852, accuracy: (0.99420625, 0.99421299)\n",
      "step: 864, loss: 0.0007538036443293095, accuracy: (0.99421299, 0.99421966)\n",
      "step: 865, loss: 0.0007526009576395154, accuracy: (0.99421966, 0.99422634)\n",
      "step: 866, loss: 0.0007514077587984502, accuracy: (0.99422634, 0.99423301)\n",
      "step: 867, loss: 0.0007501777727156878, accuracy: (0.99423301, 0.99423963)\n",
      "step: 868, loss: 0.0007490405114367604, accuracy: (0.99423963, 0.99424624)\n",
      "step: 869, loss: 0.0007477795006707311, accuracy: (0.99424624, 0.99425286)\n",
      "step: 870, loss: 0.0007465954404324293, accuracy: (0.99425286, 0.99425948)\n",
      "step: 871, loss: 0.0007454256410710514, accuracy: (0.99425948, 0.99426603)\n",
      "step: 872, loss: 0.0007442918140441179, accuracy: (0.99426603, 0.99427265)\n",
      "step: 873, loss: 0.0007430886616930366, accuracy: (0.99427265, 0.99427921)\n",
      "step: 874, loss: 0.0007418919121846557, accuracy: (0.99427921, 0.9942857)\n",
      "step: 875, loss: 0.0007407924858853221, accuracy: (0.9942857, 0.99429226)\n",
      "step: 876, loss: 0.000739550800062716, accuracy: (0.99429226, 0.99429876)\n",
      "step: 877, loss: 0.0007383777992799878, accuracy: (0.99429876, 0.99430525)\n",
      "step: 878, loss: 0.0007372272084467113, accuracy: (0.99430525, 0.99431169)\n",
      "step: 879, loss: 0.0007360684685409069, accuracy: (0.99431169, 0.99431819)\n",
      "step: 880, loss: 0.0007349711377173662, accuracy: (0.99431819, 0.99432462)\n",
      "step: 881, loss: 0.0007337955757975578, accuracy: (0.99432462, 0.99433106)\n",
      "step: 882, loss: 0.0007326578488573432, accuracy: (0.99433106, 0.9943375)\n",
      "step: 883, loss: 0.0007315250113606453, accuracy: (0.9943375, 0.99434388)\n",
      "step: 884, loss: 0.0007303173188120127, accuracy: (0.99434388, 0.99435025)\n",
      "step: 885, loss: 0.0007292883819900453, accuracy: (0.99435025, 0.99435663)\n",
      "step: 886, loss: 0.000728052924387157, accuracy: (0.99435663, 0.99436301)\n",
      "step: 887, loss: 0.0007269489578902721, accuracy: (0.99436301, 0.99436939)\n",
      "step: 888, loss: 0.0007258162368088961, accuracy: (0.99436939, 0.99437571)\n",
      "step: 889, loss: 0.0007247073808684945, accuracy: (0.99437571, 0.99438202)\n",
      "step: 890, loss: 0.0007236237870529294, accuracy: (0.99438202, 0.99438834)\n",
      "step: 891, loss: 0.0007224503206089139, accuracy: (0.99438834, 0.9943946)\n",
      "step: 892, loss: 0.000721396878361702, accuracy: (0.9943946, 0.99440092)\n",
      "step: 893, loss: 0.0007202079868875444, accuracy: (0.99440092, 0.99440718)\n",
      "step: 894, loss: 0.0007191101904027164, accuracy: (0.99440718, 0.99441344)\n",
      "step: 895, loss: 0.0007179987151175737, accuracy: (0.99441344, 0.99441963)\n",
      "step: 896, loss: 0.0007169392774812877, accuracy: (0.99441963, 0.99442589)\n",
      "step: 897, loss: 0.0007158368825912476, accuracy: (0.99442589, 0.99443209)\n",
      "step: 898, loss: 0.0007147156866267323, accuracy: (0.99443209, 0.99443829)\n",
      "step: 899, loss: 0.0007136659696698189, accuracy: (0.99443829, 0.99444443)\n",
      "step: 900, loss: 0.0007125224219635129, accuracy: (0.99444443, 0.99445063)\n",
      "step: 901, loss: 0.0007114261388778687, accuracy: (0.99445063, 0.99445677)\n",
      "step: 902, loss: 0.0007103440584614873, accuracy: (0.99445677, 0.99446291)\n",
      "step: 903, loss: 0.0007092671003192663, accuracy: (0.99446291, 0.99446905)\n",
      "step: 904, loss: 0.000708247534930706, accuracy: (0.99446905, 0.99447513)\n",
      "step: 905, loss: 0.0007071236614137888, accuracy: (0.99447513, 0.99448127)\n",
      "step: 906, loss: 0.0007060435018502176, accuracy: (0.99448127, 0.99448735)\n",
      "step: 907, loss: 0.0007050262065604329, accuracy: (0.99448735, 0.99449337)\n",
      "step: 908, loss: 0.0007038892945274711, accuracy: (0.99449337, 0.99449944)\n",
      "step: 909, loss: 0.0007029053522273898, accuracy: (0.99449944, 0.99450547)\n",
      "step: 910, loss: 0.0007017456227913499, accuracy: (0.99450547, 0.99451154)\n",
      "step: 911, loss: 0.0007007837994024158, accuracy: (0.99451154, 0.99451756)\n",
      "step: 912, loss: 0.0006996637093834579, accuracy: (0.99451756, 0.99452353)\n",
      "step: 913, loss: 0.0006986319785937667, accuracy: (0.99452353, 0.99452955)\n",
      "step: 914, loss: 0.0006975977448746562, accuracy: (0.99452955, 0.99453551)\n",
      "step: 915, loss: 0.0006965119391679764, accuracy: (0.99453551, 0.99454147)\n",
      "step: 916, loss: 0.0006955191493034363, accuracy: (0.99454147, 0.99454743)\n",
      "step: 917, loss: 0.0006944250781089067, accuracy: (0.99454743, 0.99455339)\n",
      "step: 918, loss: 0.0006933772237971425, accuracy: (0.99455339, 0.99455929)\n",
      "step: 919, loss: 0.000692337634973228, accuracy: (0.99455929, 0.99456519)\n",
      "step: 920, loss: 0.0006913198158144951, accuracy: (0.99456519, 0.99457109)\n",
      "step: 921, loss: 0.0006903361063450575, accuracy: (0.99457109, 0.99457699)\n",
      "step: 922, loss: 0.0006892757373861969, accuracy: (0.99457699, 0.99458289)\n",
      "step: 923, loss: 0.0006882302695885301, accuracy: (0.99458289, 0.99458873)\n",
      "step: 924, loss: 0.0006872699595987797, accuracy: (0.99458873, 0.99459457)\n",
      "step: 925, loss: 0.0006861768197268248, accuracy: (0.99459457, 0.99460042)\n",
      "step: 926, loss: 0.0006851740181446075, accuracy: (0.99460042, 0.99460626)\n",
      "step: 927, loss: 0.0006841480499133468, accuracy: (0.99460626, 0.9946121)\n",
      "step: 928, loss: 0.0006832043873146176, accuracy: (0.9946121, 0.99461788)\n",
      "step: 929, loss: 0.0006821467541158199, accuracy: (0.99461788, 0.99462366)\n",
      "step: 930, loss: 0.0006811481434851885, accuracy: (0.99462366, 0.99462944)\n",
      "step: 931, loss: 0.0006801749696023762, accuracy: (0.99462944, 0.99463516)\n",
      "step: 932, loss: 0.0006791219930164516, accuracy: (0.99463516, 0.99464095)\n",
      "step: 933, loss: 0.0006781874690204859, accuracy: (0.99464095, 0.99464667)\n",
      "step: 934, loss: 0.0006771270418539643, accuracy: (0.99464667, 0.99465239)\n",
      "step: 935, loss: 0.0006761450786143541, accuracy: (0.99465239, 0.99465811)\n",
      "step: 936, loss: 0.0006751453620381653, accuracy: (0.99465811, 0.99466383)\n",
      "step: 937, loss: 0.0006741948891431093, accuracy: (0.99466383, 0.9946695)\n",
      "step: 938, loss: 0.0006732153706252575, accuracy: (0.9946695, 0.99467516)\n",
      "step: 939, loss: 0.0006722122780047357, accuracy: (0.99467516, 0.99468082)\n",
      "step: 940, loss: 0.0006712597678415477, accuracy: (0.99468082, 0.99468648)\n",
      "step: 941, loss: 0.0006702366517856717, accuracy: (0.99468648, 0.99469215)\n",
      "step: 942, loss: 0.0006692575989291072, accuracy: (0.99469215, 0.99469775)\n",
      "step: 943, loss: 0.0006682851817458868, accuracy: (0.99469775, 0.99470341)\n",
      "step: 944, loss: 0.0006673217285424471, accuracy: (0.99470341, 0.99470901)\n",
      "step: 945, loss: 0.0006664023967459798, accuracy: (0.99470901, 0.99471456)\n",
      "step: 946, loss: 0.0006654038443230093, accuracy: (0.99471456, 0.99472016)\n",
      "step: 947, loss: 0.0006644352106377482, accuracy: (0.99472016, 0.99472576)\n",
      "step: 948, loss: 0.0006635228055529296, accuracy: (0.99472576, 0.99473131)\n",
      "step: 949, loss: 0.0006624964298680425, accuracy: (0.99473131, 0.99473685)\n",
      "step: 950, loss: 0.0006616183090955019, accuracy: (0.99473685, 0.99474239)\n",
      "step: 951, loss: 0.000660585006698966, accuracy: (0.99474239, 0.99474788)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 952, loss: 0.0006596672465093434, accuracy: (0.99474788, 0.99475342)\n",
      "step: 953, loss: 0.0006587104871869087, accuracy: (0.99475342, 0.9947589)\n",
      "step: 954, loss: 0.0006577832973562181, accuracy: (0.9947589, 0.99476439)\n",
      "step: 955, loss: 0.0006568605895154178, accuracy: (0.99476439, 0.99476987)\n",
      "step: 956, loss: 0.0006558864843100309, accuracy: (0.99476987, 0.99477535)\n",
      "step: 957, loss: 0.0006549918325617909, accuracy: (0.99477535, 0.99478078)\n",
      "step: 958, loss: 0.0006539954920299351, accuracy: (0.99478078, 0.99478626)\n",
      "step: 959, loss: 0.0006530735990963876, accuracy: (0.99478626, 0.99479169)\n",
      "step: 960, loss: 0.000652139657177031, accuracy: (0.99479169, 0.99479711)\n",
      "step: 961, loss: 0.0006512507097795606, accuracy: (0.99479711, 0.99480247)\n",
      "step: 962, loss: 0.0006503163604065776, accuracy: (0.99480247, 0.9948079)\n",
      "step: 963, loss: 0.0006493788678199053, accuracy: (0.9948079, 0.99481326)\n",
      "step: 964, loss: 0.0006484846817329526, accuracy: (0.99481326, 0.99481863)\n",
      "step: 965, loss: 0.0006475283298641443, accuracy: (0.99481863, 0.99482399)\n",
      "step: 966, loss: 0.0006466088816523552, accuracy: (0.99482399, 0.99482936)\n",
      "step: 967, loss: 0.0006456953706219792, accuracy: (0.99482936, 0.99483472)\n",
      "step: 968, loss: 0.0006447808118537068, accuracy: (0.99483472, 0.99484003)\n",
      "step: 969, loss: 0.0006439294666051865, accuracy: (0.99484003, 0.99484539)\n",
      "step: 970, loss: 0.0006429807981476188, accuracy: (0.99484539, 0.9948507)\n",
      "step: 971, loss: 0.0006420722929760814, accuracy: (0.9948507, 0.99485594)\n",
      "step: 972, loss: 0.0006412074435502291, accuracy: (0.99485594, 0.99486125)\n",
      "step: 973, loss: 0.000640255690086633, accuracy: (0.99486125, 0.99486655)\n",
      "step: 974, loss: 0.0006394215160980821, accuracy: (0.99486655, 0.9948718)\n",
      "step: 975, loss: 0.0006384473526850343, accuracy: (0.9948718, 0.99487704)\n",
      "step: 976, loss: 0.000637633609585464, accuracy: (0.99487704, 0.99488229)\n",
      "step: 977, loss: 0.0006366861052811146, accuracy: (0.99488229, 0.99488753)\n",
      "step: 978, loss: 0.0006358164828270674, accuracy: (0.99488753, 0.99489278)\n",
      "step: 979, loss: 0.0006349431350827217, accuracy: (0.99489278, 0.99489796)\n",
      "step: 980, loss: 0.0006340264808386564, accuracy: (0.99489796, 0.99490315)\n",
      "step: 981, loss: 0.000633184623438865, accuracy: (0.99490315, 0.99490833)\n",
      "step: 982, loss: 0.0006322655826807022, accuracy: (0.99490833, 0.99491352)\n",
      "step: 983, loss: 0.0006313793128356338, accuracy: (0.99491352, 0.9949187)\n",
      "step: 984, loss: 0.0006305009010247886, accuracy: (0.9949187, 0.99492383)\n",
      "step: 985, loss: 0.0006296382052823901, accuracy: (0.99492383, 0.99492902)\n",
      "step: 986, loss: 0.0006288021104410291, accuracy: (0.99492902, 0.99493414)\n",
      "step: 987, loss: 0.0006279141525737941, accuracy: (0.99493414, 0.99493927)\n",
      "step: 988, loss: 0.0006270263693295419, accuracy: (0.99493927, 0.99494439)\n",
      "step: 989, loss: 0.0006262154784053564, accuracy: (0.99494439, 0.99494952)\n",
      "step: 990, loss: 0.0006252858438529074, accuracy: (0.99494952, 0.99495459)\n",
      "step: 991, loss: 0.0006244435207918286, accuracy: (0.99495459, 0.99495965)\n",
      "step: 992, loss: 0.000623574887868017, accuracy: (0.99495965, 0.99496478)\n",
      "step: 993, loss: 0.0006227719131857157, accuracy: (0.99496478, 0.99496984)\n",
      "step: 994, loss: 0.0006218791240826249, accuracy: (0.99496984, 0.99497485)\n",
      "step: 995, loss: 0.0006210310384631157, accuracy: (0.99497485, 0.99497992)\n",
      "step: 996, loss: 0.0006202122895047069, accuracy: (0.99497992, 0.99498498)\n",
      "step: 997, loss: 0.0006193143781274557, accuracy: (0.99498498, 0.99498999)\n",
      "step: 998, loss: 0.00061852514045313, accuracy: (0.99498999, 0.994995)\n",
      "step: 999, loss: 0.0006176261231303215, accuracy: (0.994995, 0.995)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())  # accuracy/count is local variable\n",
    "    \n",
    "    for i in range(1000):\n",
    "        _, loss_, acc_ = sess.run([train_op, loss, acc])\n",
    "        print('step: {}, loss: {}, accuracy: {}'.format(i, loss_, acc_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
